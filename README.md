# ğŸš€ OpenAI LLM Proxy

<div align="center">

**åŸºäºLiteLLMçš„100+æ¨¡å‹ç»Ÿä¸€ä»£ç†æœåŠ¡**

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-latest-green.svg)](https://fastapi.tiangolo.com)
[![LiteLLM](https://img.shields.io/badge/LiteLLM-100%2B%20Models-orange.svg)](https://github.com/BerriAI/litellm)
[![Type Safety](https://img.shields.io/badge/Type%20Safe-mypy-blue.svg)](https://mypy.readthedocs.io)

*ä¸€é”®æ¥å…¥å…¨çƒä¸»æµAIæ¨¡å‹ï¼Œç»Ÿä¸€OpenAI APIæ ¼å¼*

</div>

## ğŸ¯ é¡¹ç›®äº®ç‚¹

### ğŸ’« **100+æ¨¡å‹æ”¯æŒ**
- **OpenAI**: GPT-4o, GPT-4 Turbo, GPT-3.5, O1ç³»åˆ—
- **Anthropic**: Claude 3.5 Sonnet, Claude 3 Opus/Haiku
- **Google**: Gemini 2.0 Flash, Gemini 1.5 Pro/Flash
- **å›½å†…å‚å•†**: æ·±åº¦æ±‚ç´¢ã€ç™¾åº¦æ–‡å¿ƒã€é˜¿é‡Œé€šä¹‰ã€å­—èŠ‚ç«å±±
- **å¼€æºæ¨¡å‹**: Llama, Mistral, Qwen é€šè¿‡å„ç§éƒ¨ç½²æ–¹å¼

### âš¡ **æè‡´æ€§èƒ½**
- **çœŸå¼‚æ­¥å¹¶å‘** - 100ä¸ªè¯·æ±‚3ç§’å†…å®Œæˆ
- **é¦–Tokenå“åº”** - å¹³å‡1.5ç§’å†…è¿”å›
- **é«˜ååé‡** - æ”¯æŒæ¯ç§’30+å¹¶å‘è¯·æ±‚

### ğŸ”§ **æç®€é…ç½®** 
- **ç¯å¢ƒå˜é‡é…ç½®** - åªéœ€è®¾ç½®å¯¹åº”çš„API Key
- **çƒ­é‡è½½é…ç½®** - è¿è¡Œæ—¶åŠ¨æ€æ·»åŠ æ¨¡å‹
- **ç»Ÿä¸€APIæ ¼å¼** - å®Œå…¨å…¼å®¹OpenAI SDK

## ğŸŒŸ æ”¯æŒçš„æ¨¡å‹æä¾›å•†

| æä¾›å•† | æ”¯æŒæ¨¡å‹ | é…ç½®å˜é‡ | æ¨èæŒ‡æ•° |
|--------|----------|----------|----------|
| ğŸ”¥ **OpenAI** | GPT-4o, GPT-4, GPT-3.5, O1 | `OPENAI_API_KEY` | â­â­â­â­â­ |
| ğŸ¤– **Anthropic** | Claude 3.5 Sonnet, Claude 3 Opus | `ANTHROPIC_API_KEY` | â­â­â­â­â­ |
| ğŸ§  **Google AI** | Gemini 2.0 Flash, Gemini 1.5 Pro | `GOOGLE_API_KEY` | â­â­â­â­â­ |
| ğŸ¯ **ç«å±±å¼•æ“** | æ·±åº¦æ±‚ç´¢V3, å­—èŠ‚è±†åŒ… | `VOLCENGINE_API_KEY` | â­â­â­â­â­ |
| ğŸ”¸ **ç™¾åº¦åƒå¸†** | æ–‡å¿ƒ4.0, ERNIE-Speed | `QIANFAN_AK`, `QIANFAN_SK` | â­â­â­â­ |
| ğŸŒ™ **é˜¿é‡Œçµç§¯** | é€šä¹‰åƒé—®Max, Plus | `DASHSCOPE_API_KEY` | â­â­â­â­ |
| âš¡ **Groq** | Llama 3.1, Mixtral | `GROQ_API_KEY` | â­â­â­â­ |
| ğŸ  **æœ¬åœ°éƒ¨ç½²** | Ollama, vLLM, TGI | æ— éœ€å¯†é’¥ | â­â­â­ |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- **Python 3.11+** 
- **uv** (æ¨èçš„ç°ä»£PythonåŒ…ç®¡ç†å™¨)

### 1. å®‰è£…uv (å¦‚æœè¿˜æ²¡æœ‰)

```bash
# macOS/Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# Windows
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

### 2. å…‹éš†å’Œå®‰è£…

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/catcc610/openai-llm-proxy.git
cd openai-llm-proxy

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…ä¾èµ–
uv sync

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ (å¯é€‰ï¼Œuv runä¼šè‡ªåŠ¨å¤„ç†)
source .venv/bin/activate  # Linux/macOS
# æˆ– .venv\Scripts\activate  # Windows
```

### 3. é…ç½®APIå¯†é’¥

ç¼–è¾‘ `config/config.yaml`ï¼š

```yaml
# ç¯å¢ƒå˜é‡é…ç½® - åªéœ€è®¾ç½®ä½ è¦ä½¿ç”¨çš„æä¾›å•†
os_env:
  # OpenAI - æœ€é«˜è´¨é‡
  OPENAI_API_KEY: "sk-your-openai-key"
  
  # Anthropic Claude - å¼ºæ¨ç†èƒ½åŠ›  
  ANTHROPIC_API_KEY: "sk-ant-your-key"
  
  # Google Gemini - æ€§ä»·æ¯”é«˜
  GOOGLE_API_KEY: "your-google-key"
  
  # ç«å±±å¼•æ“ - å›½å†…é¦–é€‰
  VOLCENGINE_API_KEY: "your-volcengine-key"
  
  # ç™¾åº¦åƒå¸†
  QIANFAN_AK: "your-qianfan-ak"
  QIANFAN_SK: "your-qianfan-sk"
  
  # é˜¿é‡Œçµç§¯
  DASHSCOPE_API_KEY: "your-dashscope-key"

# æ¨¡å‹é…ç½® - è‡ªå®šä¹‰æ¨¡å‹åç§°æ˜ å°„
model_config:
  "gpt-4o": openai                    # OpenAI GPT-4o
  "claude-3-5-sonnet": anthropic      # Anthropic Claude
  "gemini-2-flash": google            # Google Gemini  
  "deepseek-v3": volcengine           # ç«å±±å¼•æ“æ·±åº¦æ±‚ç´¢
  "qwen-max": dashscope               # é˜¿é‡Œé€šä¹‰åƒé—®
  "ernie-4": qianfan                  # ç™¾åº¦æ–‡å¿ƒä¸€è¨€

# æä¾›å•†è·¯ç”±é…ç½®
model_routes:
  openai:
    "gpt-4o": "gpt-4o"
    "gpt-4": "gpt-4"
  anthropic:
    "claude-3-5-sonnet": "claude-3-5-sonnet-20241022"
  google:
    "gemini-2-flash": "gemini-2.0-flash"
  volcengine:
    "deepseek-v3": "deepseek-v3"
  dashscope:
    "qwen-max": "qwen-max"
  qianfan:
    "ernie-4": "ERNIE-4.0-8K"
```

### 4. å¯åŠ¨æœåŠ¡

```bash
# ä½¿ç”¨uvè¿è¡Œ
uv run python main.py

# æˆ–è€…æ¿€æ´»è™šæ‹Ÿç¯å¢ƒåè¿è¡Œ
python main.py

# æœåŠ¡å¯åŠ¨åœ¨ http://localhost:9000
```

### 5. æµ‹è¯•ä½¿ç”¨

```python
from openai import OpenAI

# è¿æ¥æœ¬åœ°ä»£ç†
client = OpenAI(
    base_url="http://localhost:9000/v1",
    api_key="dummy"  # ä½¿ç”¨ä»»æ„å€¼ï¼ŒçœŸå®å¯†é’¥åœ¨æœåŠ¡ç«¯é…ç½®
)

# ä½¿ç”¨ä¸åŒæ¨¡å‹
models = [
    "gpt-4o",           # OpenAIæœ€æ–°æ¨¡å‹
    "claude-3-5-sonnet", # Anthropic Claude
    "gemini-2-flash",    # Google Gemini
    "deepseek-v3",       # ç«å±±å¼•æ“æ·±åº¦æ±‚ç´¢
    "qwen-max",          # é˜¿é‡Œé€šä¹‰åƒé—®
    "ernie-4"            # ç™¾åº¦æ–‡å¿ƒä¸€è¨€
]

for model in models:
    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": "ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}],
        max_tokens=100
    )
    print(f"{model}: {response.choices[0].message.content}")
```

## ğŸ”§ é«˜çº§é…ç½®

### é€šé…ç¬¦è·¯ç”± - æ”¯æŒæ‰€æœ‰æ¨¡å‹

```yaml
model_list:
  # OpenAI æ‰€æœ‰æ¨¡å‹
  - model_name: "openai/*"
    litellm_params:
      model: "openai/*"
      api_key: os.environ/OPENAI_API_KEY
      
  # Anthropic æ‰€æœ‰æ¨¡å‹  
  - model_name: "anthropic/*"
    litellm_params:
      model: "anthropic/*"
      api_key: os.environ/ANTHROPIC_API_KEY
      
  # Google æ‰€æœ‰æ¨¡å‹
  - model_name: "google/*"
    litellm_params:
      model: "google/*"
      api_key: os.environ/GOOGLE_API_KEY
```

### æœ¬åœ°æ¨¡å‹æ”¯æŒ

```yaml
model_list:
  # Ollama æœ¬åœ°éƒ¨ç½²
  - model_name: "llama3.1"
    litellm_params:
      model: "ollama/llama3.1"
      api_base: "http://localhost:11434"
      
  # vLLM éƒ¨ç½²
  - model_name: "custom-model"
    litellm_params:
      model: "openai/custom-model"
      api_base: "http://localhost:8000"
      api_key: "fake-key"
```

## ğŸ’» ä½¿ç”¨ç¤ºä¾‹

### Python SDK (æ¨è)

```python
import openai

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = openai.OpenAI(
    base_url="http://localhost:9000/v1",
    api_key="dummy"
)

# æ™®é€šå¯¹è¯
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹"},
        {"role": "user", "content": "è§£é‡Šä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹"}
    ]
)

# æµå¼å“åº”
stream = client.chat.completions.create(
    model="claude-3-5-sonnet",
    messages=[{"role": "user", "content": "å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")

# è§†è§‰æ¨¡å‹
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{
        "role": "user", 
        "content": [
            {"type": "text", "text": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"},
            {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,..."}}
        ]
    }]
)

# å·¥å…·è°ƒç”¨
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}],
    tools=[{
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {"type": "string", "description": "åŸå¸‚åç§°"}
                }
            }
        }
    }]
)
```

### HTTP API è°ƒç”¨

```bash
# æ™®é€šèŠå¤©
curl -X POST http://localhost:9000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "ä½ å¥½"}]
  }'

# æµå¼å“åº”
curl -X POST http://localhost:9000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-3-5-sonnet",
    "messages": [{"role": "user", "content": "å†™ä¸€é¦–è¯—"}],
    "stream": true
  }'

# è·å–æ¨¡å‹åˆ—è¡¨
curl http://localhost:9000/v1/models

# å¥åº·æ£€æŸ¥
curl http://localhost:9000/health
```

### JavaScript/Node.js

```javascript
import OpenAI from 'openai';

const client = new OpenAI({
  baseURL: 'http://localhost:9000/v1',
  apiKey: 'dummy'
});

async function chat() {
  const response = await client.chat.completions.create({
    model: 'gpt-4o',
    messages: [{ role: 'user', content: 'ä½ å¥½ï¼Œä¸–ç•Œï¼' }]
  });
  
  console.log(response.choices[0].message.content);
}

chat();
```

## ğŸ“Š æ€§èƒ½ä¼˜åŠ¿

### ğŸš€ å¼‚æ­¥ä¼˜åŒ–åçš„æ€§èƒ½è¡¨ç°

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡å€æ•° |
|------|--------|--------|----------|
| **é¦–Tokenå“åº”** | 29.4ç§’ | 1.5ç§’ | **19.6å€** |
| **å¹¶å‘å¤„ç†** | ä¸²è¡Œæ’é˜Ÿ | çœŸå¹¶å‘ | **è´¨çš„é£è·ƒ** |
| **ååé‡** | 2 QPS | 33+ QPS | **16.5å€** |
| **æ€»å“åº”æ—¶é—´** | 29.5ç§’ | 3.0ç§’ | **9.8å€** |

### ğŸ¯ æ€§èƒ½æµ‹è¯•ä»£ç 

```python
import asyncio
import aiohttp
import time

async def benchmark():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    async with aiohttp.ClientSession() as session:
        tasks = []
        start_time = time.time()
        
        # 100ä¸ªå¹¶å‘è¯·æ±‚
        for i in range(100):
            task = session.post(
                "http://localhost:9000/v1/chat/completions",
                json={
                    "model": "gpt-4o",
                    "messages": [{"role": "user", "content": f"æµ‹è¯• {i+1}"}],
                    "max_tokens": 50
                }
            )
            tasks.append(task)
        
        responses = await asyncio.gather(*tasks)
        total_time = time.time() - start_time
        
        print(f"100ä¸ªå¹¶å‘è¯·æ±‚å®Œæˆæ—¶é—´: {total_time:.2f}ç§’")
        print(f"å¹³å‡QPS: {100/total_time:.1f}")

asyncio.run(benchmark())
```

## ğŸ› ï¸ ç®¡ç†åŠŸèƒ½

### é…ç½®çƒ­é‡è½½

```bash
# é‡è½½é…ç½®æ–‡ä»¶
curl -X POST http://localhost:9000/config/reload

# æŸ¥çœ‹å½“å‰é…ç½®
curl http://localhost:9000/config
```

### æ¨¡å‹ç®¡ç†

```bash
# è·å–æ”¯æŒçš„æ‰€æœ‰æ¨¡å‹
curl http://localhost:9000/v1/models

# è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯
curl http://localhost:9000/model_group/info
```

### æ—¥å¿—ç›‘æ§

```python
# å¯åŠ¨æ—¶å¼€å¯è°ƒè¯•æ—¥å¿—
uv run python main.py --log-level debug

# æŸ¥çœ‹è¯·æ±‚æ—¥å¿—
tail -f logs/app.log
```

## ğŸ¨ é¡¹ç›®ç‰¹è‰²

### ğŸ—ï¸ ç°ä»£åŒ–æ¶æ„
- **FastAPI** - é«˜æ€§èƒ½å¼‚æ­¥Webæ¡†æ¶
- **Pydantic** - ä¸¥æ ¼çš„ç±»å‹éªŒè¯å’Œåºåˆ—åŒ–
- **ç±»å‹å®‰å…¨** - å®Œæ•´çš„MyPyç±»å‹æ£€æŸ¥

### ğŸ”’ ä¼ä¸šçº§ç‰¹æ€§
- **è¯·æ±‚è¿½è¸ª** - æ¯ä¸ªè¯·æ±‚å”¯ä¸€IDï¼Œä¾¿äºè°ƒè¯•
- **é”™è¯¯å¤„ç†** - ä¼˜é›…çš„å¼‚å¸¸å¤„ç†å’Œé”™è¯¯ä¿¡æ¯
- **é…ç½®ç®¡ç†** - æ”¯æŒç¯å¢ƒå˜é‡å’ŒYAMLé…ç½®
- **å¥åº·ç›‘æ§** - å†…ç½®å¥åº·æ£€æŸ¥ç«¯ç‚¹

### ğŸŒˆ å¼€å‘ä½“éªŒ
- **é›¶å­¦ä¹ æˆæœ¬** - å®Œå…¨å…¼å®¹OpenAI APIæ ¼å¼
- **å³æ’å³ç”¨** - ç°æœ‰ä»£ç æ— éœ€ä¿®æ”¹
- **çµæ´»é…ç½®** - æ”¯æŒå„ç§éƒ¨ç½²æ–¹å¼

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### ğŸ’¼ ä¼ä¸šåº”ç”¨
- **å¤šæ¨¡å‹å¯¹æ¯”** - åŒæ—¶æµ‹è¯•ä¸åŒæ¨¡å‹æ•ˆæœ
- **æˆæœ¬ä¼˜åŒ–** - æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©æœ€ä¼˜æ¨¡å‹
- **é£é™©åˆ†æ•£** - å¤šæä¾›å•†é¿å…å•ç‚¹æ•…éšœ

### ğŸ”¬ ç ”ç©¶å¼€å‘
- **æ¨¡å‹è¯„ä¼°** - ç»Ÿä¸€æ¥å£æµ‹è¯•å„ç§æ¨¡å‹
- **åŸå‹å¼€å‘** - å¿«é€Ÿåˆ‡æ¢ä¸åŒèƒ½åŠ›çš„æ¨¡å‹
- **æ€§èƒ½æµ‹è¯•** - å¯¹æ¯”ä¸åŒæ¨¡å‹çš„å“åº”é€Ÿåº¦

### ğŸ  ä¸ªäººé¡¹ç›®
- **æˆæœ¬æ§åˆ¶** - çµæ´»é€‰æ‹©æ€§ä»·æ¯”æœ€é«˜çš„æ¨¡å‹
- **åŠŸèƒ½é›†æˆ** - ä¸€å¥—ä»£ç æ”¯æŒæ‰€æœ‰ä¸»æµæ¨¡å‹
- **å­¦ä¹ å®éªŒ** - ä½“éªŒä¸åŒAIæ¨¡å‹çš„ç‰¹è‰²

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| ğŸš« æ¨¡å‹æœªæ‰¾åˆ° | é…ç½®æ˜ å°„é”™è¯¯ | æ£€æŸ¥`model_config`é…ç½® |
| ğŸ”‘ APIå¯†é’¥æ— æ•ˆ | ç¯å¢ƒå˜é‡æœªè®¾ç½® | ç¡®è®¤`os_env`ä¸­çš„å¯†é’¥ |
| â° è¯·æ±‚è¶…æ—¶ | ç½‘ç»œæˆ–æœåŠ¡å•†é—®é¢˜ | è°ƒæ•´`timeout`è®¾ç½® |
| ğŸŒ å“åº”ç¼“æ…¢ | åŒæ­¥è°ƒç”¨é˜»å¡ | æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å¼‚æ­¥ç‰ˆæœ¬ |

### è°ƒè¯•æ–¹æ³•

```bash
# å¯åŠ¨è°ƒè¯•æ¨¡å¼
uv run python main.py --log-level debug

# æŸ¥çœ‹é…ç½®çŠ¶æ€  
curl http://localhost:9000/config

# æµ‹è¯•ç‰¹å®šæ¨¡å‹
curl -X POST http://localhost:9000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "gpt-4o", "messages": [{"role": "user", "content": "test"}]}'
```

## ğŸš€ ç”Ÿäº§éƒ¨ç½²

### ç³»ç»ŸæœåŠ¡éƒ¨ç½²

åˆ›å»ºsystemdæœåŠ¡æ–‡ä»¶ `/etc/systemd/system/llm-proxy.service`ï¼š

```ini
[Unit]
Description=LLM Proxy Service
After=network.target

[Service]
Type=simple
User=your-user
WorkingDirectory=/path/to/openai-llm-proxy
Environment=PATH=/path/to/openai-llm-proxy/.venv/bin
ExecStart=/path/to/openai-llm-proxy/.venv/bin/python main.py --host 0.0.0.0
Restart=always
RestartSec=3

[Install]
WantedBy=multi-user.target
```

```bash
# å¯ç”¨å¹¶å¯åŠ¨æœåŠ¡
sudo systemctl enable llm-proxy
sudo systemctl start llm-proxy
sudo systemctl status llm-proxy
```

### Nginxåå‘ä»£ç†

```nginx
upstream llm_proxy {
    server 127.0.0.1:9000;
}

server {
    listen 80;
    server_name api.yourdomain.com;
    
    location / {
        proxy_pass http://llm_proxy;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_buffering off;  # é‡è¦ï¼šæ”¯æŒæµå¼å“åº”
        proxy_read_timeout 300s;
    }
}
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

### å¼€å‘ç¯å¢ƒ

```bash
# å®‰è£…å¼€å‘ä¾èµ–
uv sync --group dev

# ä»£ç è´¨é‡æ£€æŸ¥
uv run ruff check .         # ä»£ç é£æ ¼æ£€æŸ¥
uv run mypy .              # ç±»å‹æ£€æŸ¥
```

### ä»£ç è´¨é‡å·¥å…·

é¡¹ç›®åŒ…å«å®Œæ•´çš„ç±»å‹å®‰å…¨å’Œä»£ç è´¨é‡é…ç½®ï¼š

- **MyPy** - é™æ€ç±»å‹æ£€æŸ¥ (`mypy.ini`)
- **Ruff** - å¿«é€Ÿçš„ä»£ç æ£€æŸ¥å’Œæ ¼å¼åŒ–
- **UV** - ç°ä»£åŒ–çš„PythonåŒ…ç®¡ç†

### æäº¤è§„èŒƒ

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯: `git checkout -b feature/amazing-feature`
3. æäº¤æ›´æ”¹: `git commit -m 'Add amazing feature'`
4. æ¨é€åˆ†æ”¯: `git push origin feature/amazing-feature`
5. åˆ›å»ºPull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT è®¸å¯è¯](LICENSE) - è¯¦è§LICENSEæ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

- [LiteLLM](https://github.com/BerriAI/litellm) - å‡ºè‰²çš„å¤šæä¾›å•†LLMç»Ÿä¸€æ¥å£åº“
- [FastAPI](https://fastapi.tiangolo.com/) - ç°ä»£åŒ–é«˜æ€§èƒ½Webæ¡†æ¶
- [Pydantic](https://pydantic.dev/) - å¼ºå¤§çš„æ•°æ®éªŒè¯å’Œç±»å‹å®‰å…¨åº“
- [UV](https://github.com/astral-sh/uv) - ç°ä»£åŒ–çš„PythonåŒ…ç®¡ç†å™¨

## ğŸ’¬ è”ç³»æ–¹å¼

- ğŸ› é—®é¢˜åé¦ˆ: [GitHub Issues](../../issues)
- ğŸ’¡ åŠŸèƒ½å»ºè®®: [GitHub Discussions](../../discussions)

---

<div align="center">

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªæ˜Ÿæ ‡ï¼**

*è®©AIæ¨¡å‹åˆ‡æ¢åƒæ¢è¡£æœä¸€æ ·ç®€å•*

</div>
