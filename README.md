# ğŸš€ LLM Proxy: ç»Ÿä¸€å¤§æ¨¡å‹ API ç½‘å…³

<div align="center">

**ä¸€è¡Œä»£ç ï¼Œç»Ÿä¸€è°ƒç”¨å…¨çƒ100+å¤§æ¨¡å‹**

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-latest-green.svg)](https://fastapi.tiangolo.com)
[![LiteLLM](https://img.shields.io/badge/LiteLLM-100%2B%20Models-orange.svg)](https://github.com/BerriAI/litellm)
[![Type Safety](https://img.shields.io/badge/Type%20Safe-mypy-blue.svg)](https://mypy.readthedocs.io)

*åŸºäº LiteLLM å’Œ FastAPI æ„å»ºï¼Œæä¾›ä¸ OpenAI API å®Œå…¨å…¼å®¹çš„ç»Ÿä¸€æ¥å£*

</div>

## ğŸ¯ é¡¹ç›®ç‰¹ç‚¹

- **âš¡ ç»Ÿä¸€API**ï¼šå®Œå…¨å…¼å®¹ OpenAI SDK å’Œ API æ ¼å¼ï¼Œæ— ç¼åˆ‡æ¢æ¨¡å‹ã€‚
- **ğŸ”Œ å¤šæ¨¡å‹æ”¯æŒ**ï¼šé€šè¿‡ LiteLLM é›†æˆè¶…è¿‡100ç§æ¨¡å‹ï¼ŒåŒ…æ‹¬ OpenAI, Anthropic, Google Gemini, ä»¥åŠå›½å†…ä¸»æµæ¨¡å‹ã€‚
- **âš™ï¸ é›†ä¸­åŒ–é…ç½®**ï¼šé€šè¿‡å•ä¸ª `config.yaml` æ–‡ä»¶ç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ¨¡å‹çš„APIå¯†é’¥å’Œè·¯ç”±è§„åˆ™ã€‚
- **ğŸš€ é«˜æ€§èƒ½**ï¼šåŸºäº FastAPI çš„å…¨å¼‚æ­¥æ¶æ„ï¼Œä¸ºé«˜å¹¶å‘åœºæ™¯æä¾›é«˜ååé‡å’Œä½å»¶è¿Ÿã€‚
- **ğŸ›¡ï¸ ç±»å‹å®‰å…¨**ï¼šä½¿ç”¨ Pydantic å’Œ mypy å¼ºåˆ¶æ‰§è¡Œä¸¥æ ¼çš„ç±»å‹æ£€æŸ¥ï¼Œä¿è¯ä»£ç çš„å¥å£®æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚

## ğŸŒŸ æ”¯æŒçš„éƒ¨åˆ†æ¨¡å‹æä¾›å•†

| æä¾›å•† | æ”¯æŒæ¨¡å‹ | é…ç½®å˜é‡ |
|--------|----------|----------|
| ğŸ”¥ **OpenAI** | GPT-4o, GPT-4, GPT-3.5 | `OPENAI_API_KEY` |
| ğŸ¤– **Anthropic** | Claude 3.5 Sonnet, Opus | `ANTHROPIC_API_KEY` |
| ğŸ§  **Google AI** | Gemini 1.5 Pro, Flash | `GOOGLE_API_KEY` |
| ğŸ¯ **ç«å±±å¼•æ“** | æ·±åº¦æ±‚ç´¢, è±†åŒ… | `VOLCENGINE_API_KEY` |
| ğŸ  **æœ¬åœ°éƒ¨ç½²** | Ollama, vLLM, TGI | (æŸ¥çœ‹é«˜çº§é…ç½®) |

> **é‡è¦æç¤º**: æä¾›å•†åç§°å’Œæ‰€éœ€çš„ç¯å¢ƒå˜é‡ç”± LiteLLM å®šä¹‰ã€‚ä¸ºäº†ç¡®ä¿é…ç½®æ­£ç¡®ï¼Œè¯·åœ¨æ·»åŠ æ–°æ¨¡å‹å‰æŸ¥é˜… [**LiteLLM å®˜æ–¹æ”¯æŒçš„æ¨¡å‹æä¾›å•†æ–‡æ¡£**](https://docs.litellm.ai/docs/providers)ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- **Python 3.11+**
- **uv** (æ¨èçš„ç°ä»£PythonåŒ…ç®¡ç†å™¨)

### 1. å®‰è£… uv

å¦‚æœä½ çš„ç³»ç»Ÿä¸­è¿˜æ²¡æœ‰ `uv`ï¼Œè¯·å…ˆæ‰§è¡Œå®‰è£…ï¼š

```bash
# macOS / Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# Windows (PowerShell)
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

### 2. å…‹éš†ä¸å®‰è£…ä¾èµ–

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/catcc610/openai-llm-proxy.git
cd openai-llm-proxy

# ä½¿ç”¨ uv åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…ä¾èµ–
uv sync

# (å¯é€‰) æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# source .venv/bin/activate  # Linux/macOS
# .venv\Scripts\activate      # Windows
```

### 3. é…ç½®æ¨¡å‹

ç¼–è¾‘ `config/config.yaml` æ–‡ä»¶ã€‚è¿™æ˜¯ä½ ç®¡ç†æ‰€æœ‰æ¨¡å‹å¯†é’¥å’Œè·¯ç”±çš„æ ¸å¿ƒä½ç½®ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªåŸºç¡€é…ç½®ç¤ºä¾‹ï¼Œæ¼”ç¤ºå¦‚ä½•æ·»åŠ  GPT-4o å’Œ Claude 3.5 Sonnetï¼š

```yaml
# 1. åœ¨ os_env éƒ¨åˆ†ï¼Œä¸ºä½ éœ€è¦ä½¿ç”¨çš„æ¨¡å‹æä¾›å•†è®¾ç½® API å¯†é’¥ã€‚
#    è¿™äº›å€¼å°†è¢«åŠ è½½ä¸ºç¯å¢ƒå˜é‡ã€‚
os_env:
  OPENAI_API_KEY: "sk-your-openai-key"
  ANTHROPIC_API_KEY: "sk-ant-your-key"

# 2. åœ¨ model_config éƒ¨åˆ†ï¼Œå°†ä½ å¸Œæœ›åœ¨APIä¸­ä½¿ç”¨çš„è‡ªå®šä¹‰æ¨¡å‹åç§°æ˜ å°„åˆ°æä¾›å•†ã€‚
#    è¿™æ˜¯å‘Šè¯‰ä»£ç†"å½“æˆ‘è¯·æ±‚'gpt-4o'æ—¶ï¼Œä½ åº”è¯¥ä½¿ç”¨'openai'è¿™ä¸ªæä¾›å•†çš„é…ç½®"ã€‚
model_config:
  "gpt-4o": openai
  "claude-3.5-sonnet": anthropic

# 3. åœ¨ model_routes éƒ¨åˆ†ï¼Œä¸ºæ¯ä¸ªæä¾›å•†å®šä¹‰å…·ä½“çš„æ¨¡å‹IDã€‚
#    è¿™ä¼šå°†ä½ çš„è‡ªå®šä¹‰åç§°æ˜ å°„åˆ°LiteLLMæ‰€éœ€çš„å®é™…æ¨¡å‹åç§°ã€‚
model_routes:
  openai:
    "gpt-4o": "gpt-4o-2024-08-06"  # LiteLLM éœ€è¦çš„å®é™…æ¨¡å‹ID
  anthropic:
    "claude-3.5-sonnet": "claude-3-5-sonnet-20240620"
```

> **ä¸ºä»€ä¹ˆéœ€è¦è¿™æ ·é…ç½®ï¼Ÿ**
>
> è¿™ç§ä¸‰æ®µå¼é…ç½®æä¾›äº†ä¸€ç§çµæ´»çš„è·¯ç”±æœºåˆ¶ï¼š
> - `os_env` é›†ä¸­ç®¡ç†å¯†é’¥ã€‚
> - `model_config` å…è®¸ä½ ä½¿ç”¨ç®€æ´çš„è‡ªå®šä¹‰åç§°ï¼ˆå¦‚ `gpt-4o`ï¼‰ä½œä¸ºAPIå…¥å£ã€‚
> - `model_routes` åˆ™å°†è¿™äº›åç§°ç²¾ç¡®æ˜ å°„åˆ°ä¸åŒæä¾›å•†ä¸æ–­æ›´æ–°çš„å®˜æ–¹æ¨¡å‹IDä¸Šï¼Œè€Œæ— éœ€ä¿®æ”¹ä½ çš„å®¢æˆ·ç«¯ä»£ç ã€‚

### 4. å¯åŠ¨æœåŠ¡

```bash
# ä½¿ç”¨ uv ç›´æ¥è¿è¡Œ
uv run python main.py

# æœåŠ¡å°†å¯åŠ¨åœ¨ http://localhost:9000
```

### 5. æµ‹è¯•è°ƒç”¨

ä½¿ç”¨ä½ å–œæ¬¢çš„HTTPå®¢æˆ·ç«¯æˆ–OpenAIå®˜æ–¹SDKè¿›è¡Œæµ‹è¯•ã€‚

```python
from openai import OpenAI

# è¿æ¥åˆ°æœ¬åœ°ä»£ç†æœåŠ¡
client = OpenAI(
    base_url="http://localhost:9000/v1",
    api_key="any-key"  # ä»£ç†æœåŠ¡ç«¯çš„å¯†é’¥æ‰æ˜¯å…³é”®ï¼Œè¿™é‡Œå¯å¡«ä»»æ„å€¼
)

# ä½¿ç”¨ä½ åœ¨ config.yaml ä¸­å®šä¹‰çš„æ¨¡å‹åç§°
models_to_test = ["gpt-4o", "claude-3-5-sonnet"]

for model_name in models_to_test:
    try:
        print(f"--- æ­£åœ¨æµ‹è¯•æ¨¡å‹: {model_name} ---")
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}],
            max_tokens=100
        )
        print(f"å“åº”: {response.choices[0].message.content}\n")
    except Exception as e:
        print(f"è°ƒç”¨æ¨¡å‹ {model_name} æ—¶å‡ºé”™: {e}\n")
```

## ğŸ”§ é«˜çº§é…ç½®

### æœ¬åœ°æ¨¡å‹ (Ollama)

ä½ å¯ä»¥é…ç½®ä»£ç†ä»¥è¿æ¥åˆ°æœ¬åœ°è¿è¡Œçš„æ¨¡å‹ï¼Œä¾‹å¦‚é€šè¿‡Ollamaéƒ¨ç½²çš„Llama 3.1ã€‚

```yaml
# config/config.yaml

# 1. os_env ä¸­æ— éœ€æ·»åŠ å¯†é’¥ (å¯¹äºæœ¬åœ°Ollama)

# 2. model_config ä¸­æ˜ å°„æ¨¡å‹åç§°åˆ°è‡ªå®šä¹‰çš„æä¾›å•†åç§° "ollama_local"
model_config:
  "llama3.1": ollama_local

# 3. model_routes ä¸­å®šä¹‰ "ollama_local" æä¾›å•†çš„å…·ä½“é…ç½®
model_routes:
  ollama_local:
    # è¿™é‡Œçš„ "llama3.1" å¿…é¡»ä¸ model_config ä¸­çš„åç§°åŒ¹é…
    "llama3.1": "ollama/llama3.1" # LiteLLMæ ¼å¼: "ollama/<model_tag>"
```
> **æ³¨æ„**: ä¸Šè¿°é…ç½®ä¸­çš„ `ollama_local` æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰çš„æä¾›å•†æ ‡è¯†ç¬¦ï¼Œä½ å¯ä»¥ä½¿ç”¨ä»»ä½•ä½ å–œæ¬¢çš„åç§°ï¼Œåªè¦åœ¨ `model_config` å’Œ `model_routes` ä¸­ä¿æŒä¸€è‡´å³å¯ã€‚LiteLLMå°†æ ¹æ® `ollama/` å‰ç¼€è¯†åˆ«å¹¶è¿æ¥åˆ°é»˜è®¤çš„OllamaæœåŠ¡åœ°å€ (`http://localhost:11434`)ã€‚

## ğŸ’» API ä½¿ç”¨ç¤ºä¾‹

ä»£ç†æœåŠ¡å®Œå…¨å…¼å®¹OpenAIçš„APIè§„èŒƒã€‚ä½ å¯ä»¥ä½¿ç”¨ä»»ä½•æ”¯æŒOpenAI APIçš„å·¥å…·ã€‚

### Python SDK

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:9000/v1",
    api_key="dummy-key"
)

# --- åŸºç¡€å¯¹è¯ ---
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯"é‡å­çº ç¼ "ã€‚"}
    ]
)
print(response.choices[0].message.content)

# --- æµå¼å“åº” ---
stream = client.chat.completions.create(
    model="claude-3-5-sonnet",
    messages=[{"role": "user", "content": "ç”¨Pythonå†™ä¸€ä¸ªæ–æ³¢é‚£å¥‘æ•°åˆ—å‡½æ•°ï¼Œå¹¶è§£é‡Šå…¶å·¥ä½œåŸç†ã€‚"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)

# --- è§†è§‰æ¨¡å‹ï¼ˆå¤šæ¨¡æ€ï¼‰ ---
response = client.chat.completions.create(
    model="gpt-4o", # ç¡®ä¿æ­¤æ¨¡å‹æ”¯æŒè§†è§‰
    messages=[{
        "role": "user", 
        "content": [
            {"type": "text", "text": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆå†…å®¹ï¼Ÿ"},
            {
                "type": "image_url", 
                "image_url": {
                    # æ”¯æŒURLæˆ–Base64ç¼–ç çš„å›¾ç‰‡
                    "url": "data:image/jpeg;base64,/9j/4AAQSk...your_base64_string...",
                }
            }
        ]
    }]
)
print(response.choices[0].message.content)
```

### cURL

```bash
# åŸºç¡€å¯¹è¯
curl http://localhost:9000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o",
    "messages": [{"role": "user", "content": "ä½ å¥½ï¼"}]
  }'

# æµå¼å“åº”
curl http://localhost:9000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-3-5-sonnet",
    "messages": [{"role": "user", "content": "å†™ä¸€é¦–å…³äºå®‡å®™çš„çŸ­è¯—ã€‚"}],
    "stream": true
  }'

# è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ (åŸºäºä½ çš„é…ç½®)
curl http://localhost:9000/v1/models

# å¥åº·æ£€æŸ¥
curl http://localhost:9000/health
```

## ğŸ§ª æ€§èƒ½æµ‹è¯•

æœ¬é¡¹ç›®åŸºäºå¼‚æ­¥æ¡†æ¶æ„å»ºï¼Œèƒ½å¤Ÿå¤„ç†é«˜å¹¶å‘è¯·æ±‚ã€‚ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è„šæœ¬è¿›è¡Œç®€å•çš„åŸºå‡†æµ‹è¯•ã€‚

```python
import asyncio
import aiohttp
import time

# æµ‹è¯•å‚æ•°
CONCURRENT_REQUESTS = 100
MODEL_TO_TEST = "gpt-4o" # æ›¿æ¢ä¸ºä½ æƒ³æµ‹è¯•çš„ã€å·²é…ç½®çš„æ¨¡å‹
PROMPT = "ä½ å¥½"

async def benchmark():
    """å¯¹ä»£ç†æœåŠ¡è¿›è¡Œå¹¶å‘è¯·æ±‚åŸºå‡†æµ‹è¯•"""
    async with aiohttp.ClientSession() as session:
        tasks = []
        start_time = time.time()
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        for i in range(CONCURRENT_REQUESTS):
            task = session.post(
                "http://localhost:9000/v1/chat/completions",
                json={
                    "model": MODEL_TO_TEST,
                    "messages": [{"role": "user", "content": f"{PROMPT} {i+1}"}],
                    "max_tokens": 50
                },
                headers={"Authorization": "Bearer dummy-key"}
            )
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
        responses = await asyncio.gather(*[asyncio.ensure_future(t) for t in tasks])
        
        successful_requests = [r for r in responses if r.status == 200]
        end_time = time.time()
        total_time = end_time - start_time
        
        print(f"--- æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ ---")
        print(f"æµ‹è¯•æ¨¡å‹: {MODEL_TO_TEST}")
        print(f"æ€»è¯·æ±‚æ•°: {CONCURRENT_REQUESTS}")
        print(f"æˆåŠŸè¯·æ±‚æ•°: {len(successful_requests)}")
        print(f"æ€»è€—æ—¶: {total_time:.2f} ç§’")
        
        if total_time > 0:
            qps = len(successful_requests) / total_time
            print(f"å¹³å‡QPS (æ¯ç§’è¯·æ±‚æ•°): {qps:.2f}")

if __name__ == "__main__":
    asyncio.run(benchmark())
```