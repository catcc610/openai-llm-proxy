"""
External LLM Service - æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
"""

from __future__ import annotations
import json
import uuid
import asyncio
from typing import Any, AsyncGenerator, Dict, Union
import time
from fastapi import Request
from fastapi.responses import StreamingResponse
from litellm import acompletion
from litellm.utils import ModelResponse

from app.services.base import BaseService, ServiceError
from app.services.external_llm.router import (
    get_provider_from_model,
    resolve_model,
)
from app.services.external_llm.provider_manager import ProviderManager
from logger.logger import get_logger
from config.config import get_external_llm_config

logger = get_logger(__name__)


class ExternalLLMService(BaseService):
    """å¤„ç†ä¸å¤–éƒ¨ LLM æä¾›å•†äº¤äº’çš„æ ¸å¿ƒæœåŠ¡"""

    def __init__(self) -> None:
        super().__init__("external_llm", get_external_llm_config())
        self.provider_manager = ProviderManager()
        logger.info("âœ… External LLM æœåŠ¡å·²æˆåŠŸåˆå§‹åŒ–")

    def get_models_info(self) -> list[dict[str, Any]]:
        """
        è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹çš„ä¿¡æ¯ï¼Œæ ¼å¼éµå¾ª OpenAI /models æ¥å£
        """
        config = get_external_llm_config()
        try:
            models = []
            for model_id, provider in config.get("provider_config", {}).items():
                models.append(
                    {
                        "id": model_id,
                        "object": "model",
                        "created": 1677610602,
                        "owned_by": f"llm-proxy-{provider}",
                        "root": model_id,
                        "parent": None,
                        "permission": [
                            {
                                "id": f"modelperm-{model_id}",
                                "object": "model_permission",
                                "created": 1677610602,
                                "allow_create_engine": False,
                                "allow_sampling": True,
                                "allow_logprobs": True,
                                "allow_search_indices": False,
                                "allow_view": True,
                                "allow_fine_tuning": False,
                                "organization": "*",
                                "group": None,
                                "is_blocking": False,
                            }
                        ],
                    }
                )
            return sorted(models, key=lambda x: str(x["id"]))
        except Exception as e:
            logger.error(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            raise ServiceError(
                message="è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥", error_code="MODEL_LIST_ERROR"
            )

    async def handle_chat_completion(
        self, payload: Dict[str, Any], request: Request
    ) -> Union[StreamingResponse, Dict[str, Any]]:
        """
        å¤„ç†èŠå¤©å®Œæˆè¯·æ±‚ï¼Œé€šè¿‡LiteLLMè½¬å‘åˆ°ç›¸åº”çš„LLMæä¾›å•†
        """
        request_id = (
            request.headers.get("X-Request-ID")
            or getattr(request.state, "request_id", None)
            or str(uuid.uuid4())
        )
        request.state.request_id = request_id

        try:
            t0 = time.time()
            payload = await request.json()
            t1 = time.time()

            model = payload.get("model")
            if not model:
                raise ServiceError(
                    message="è¯·æ±‚ä½“ä¸­ç¼ºå°‘ 'model' å­—æ®µ",
                    error_code="VALIDATION_ERROR",
                )

            stream = payload.get("stream", False)
            logger.info(
                f"ğŸ“¥ [{request_id}] æ”¶åˆ°èŠå¤©è¯·æ±‚: model={model}, stream={stream}"
            )

            litellm_params = self._prepare_litellm_params(payload, request_id)
            t2 = time.time()
            logger.debug(f"ğŸš€ [{request_id}] å¼€å§‹è°ƒç”¨LiteLLM...")

            response = await acompletion(**litellm_params)
            logger.debug(f"âœ… [{request_id}] LiteLLMè°ƒç”¨æˆåŠŸ")

            logger.info(
                f"â±ï¸ [{request_id}] è§£æè¯·æ±‚è€—æ—¶: {(t1 - t0):.3f} ç§’, å‚æ•°å‡†å¤‡è€—æ—¶: {(t2 - t1):.6f} ç§’"
            )

            if stream:
                return await self._handle_streaming_response(
                    response, model, request_id, t2
                )
            else:
                t4 = time.time()
                result = await self._convert_to_response_dict(
                    response, model, request_id
                )
                t5 = time.time()
                logger.info(
                    f"â±ï¸ [{request_id}] å“åº”è½¬æ¢è€—æ—¶: {(t5 - t4):.3f} ç§’, æ€»è€—æ—¶: {(t5 - t0):.3f} ç§’"
                )
                return result

        except json.JSONDecodeError as e:
            logger.error(f"âŒ [{request_id}] JSONè§£æå¤±è´¥: {e}")
            raise ServiceError(
                message="æ— æ•ˆçš„JSONæ ¼å¼", error_code="VALIDATION_JSON_ERROR"
            )
        except Exception as e:
            logger.error(
                "âŒ [{request_id}] èŠå¤©å®Œæˆå¤„ç†å¼‚å¸¸: {error_details}",
                request_id=request_id,
                error_details=e,
                exc_info=True,
            )
            raise ServiceError(message=str(e), error_code="CHAT_COMPLETION_ERROR")

    def _prepare_litellm_params(
        self, payload: Dict[str, Any], request_id: str
    ) -> Dict[str, Any]:
        """
        Prepares parameters for LiteLLM by delegating to the appropriate provider handler.
        """
        config = get_external_llm_config()
        model_name = payload.get("model")

        if not model_name or "messages" not in payload:
            raise ServiceError(
                message="Request body must contain 'model' and 'messages'",
                error_code="VALIDATION_REQUEST_ERROR",
            )

        # 1. Determine the provider and the actual model name for LiteLLM
        provider_name = get_provider_from_model(model_name, config)
        model_route = resolve_model(model_name, config)

        # Determine the base model name from the route config
        if isinstance(model_route, dict):
            # For complex routes (like Vertex), get the model name from the dict
            base_model_name = model_route.get("model") or model_name
        else:
            # For simple string routes, the route is the model name
            base_model_name = model_route

        # Always prepend the provider for LiteLLM, e.g., 'vertex_ai/gemini-pro'
        litellm_model = f"{provider_name}/{base_model_name}"
        logger.debug(f"Constructed final LiteLLM model ID: {litellm_model}")

        # 2. Get the specific provider handler from our factory
        provider_handler = self.provider_manager.get_provider(provider_name, config)

        # 3. Prepare a base payload for the handler
        base_payload = payload.copy()
        base_payload["model"] = litellm_model
        base_payload["drop_params"] = True

        # 4. Delegate the final parameter preparation to the handler
        final_params = provider_handler.prepare_litellm_params(
            base_payload, model_route
        )

        logger.debug(
            f"ğŸ” [{request_id}] Final LiteLLM params from '{provider_handler.__class__.__name__}': {final_params}"
        )
        return final_params

    async def _convert_to_response_dict(
        self, litellm_response: ModelResponse, original_model: str, request_id: str
    ) -> Dict[str, Any]:
        """å°†éæµå¼LiteLLMå“åº”è½¬æ¢ä¸ºOpenAIæ ¼å¼çš„å­—å…¸"""
        # Run synchronous, CPU-bound operation in a separate thread
        try:
            response_dict = await asyncio.to_thread(litellm_response.model_dump)
            response_dict["model"] = original_model

            # æ‰“å°å“åº”ID
            if response_id := response_dict.get("id"):
                logger.info(f"ğŸ†” [{request_id}] å“åº”ID: {response_id}")

            if usage := response_dict.get("usage"):
                logger.info(f"ğŸ“Š [{request_id}] Token usage: {usage}")

            return response_dict
        except Exception as e:
            logger.error(f"âŒ [{request_id}] å“åº”è½¬æ¢å¤±è´¥: {e}")
            raise ServiceError("å“åº”æ ¼å¼è½¬æ¢å¤±è´¥", "RESPONSE_CONVERSION_ERROR")

    async def _handle_streaming_response(
        self,
        litellm_stream: Any,
        original_model: str,
        request_id: str,
        start_time: float,
    ) -> StreamingResponse:
        """å¤„ç†æµå¼å“åº”"""

        async def generate_stream() -> AsyncGenerator[str, None]:
            final_usage = {}
            first_token_time = None
            response_id_logged = False  # æ ‡å¿—ä½ï¼Œç¡®ä¿å“åº”IDåªæ‰“å°ä¸€æ¬¡
            try:
                async for chunk in litellm_stream:
                    if first_token_time is None:
                        first_token_time = time.time()
                        elapsed = first_token_time - start_time
                        logger.info(
                            f"â±ï¸ [{request_id}] é¦– token å“åº”è€—æ—¶: {elapsed:.3f} ç§’"
                        )
                    chunk_dict = chunk.model_dump()
                    chunk_dict["model"] = original_model

                    # åœ¨ç¬¬ä¸€ä¸ªchunkæ‰“å°å“åº”ID
                    if not response_id_logged:
                        if response_id := chunk_dict.get("id"):
                            logger.info(
                                f"ğŸ†” [{request_id}] å“åº”ID (stream): {response_id}"
                            )
                            response_id_logged = True

                    if usage := chunk_dict.get("usage"):
                        final_usage = usage

                    yield f"data: {json.dumps(chunk_dict)}\n\n"
            except Exception as e:
                logger.error(f"âŒ [{request_id}] æµå¤„ç†å¼‚å¸¸: {e}")
                error_info = {
                    "error": {"message": f"æµå¤„ç†é”™è¯¯: {e}", "type": "STREAM_ERROR"}
                }
                yield f"data: {json.dumps(error_info)}\n\n"
            finally:
                if final_usage:
                    logger.info(
                        f"ğŸ“Š [{request_id}] Token usage (stream): {final_usage}"
                    )
                yield "data: [DONE]\n\n"

        return StreamingResponse(generate_stream(), media_type="text/event-stream")

    def get_all_provider_stats(self) -> Any:
        """è·å–æ‰€æœ‰æä¾›å•†çš„çŠ¶æ€"""
        config = get_external_llm_config()
        return self.provider_manager.get_all_provider_stats(config)

    async def get_models(self) -> list[dict[str, Any]]:
        """
        è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹, æ ¼å¼éµå¾ªOpenAIè§„èŒƒã€‚
        """
        config = get_external_llm_config()
        try:
            models = []
            for model_id, provider in config.get("provider_config", {}).items():
                models.append(
                    {
                        "id": model_id,
                        "object": "model",
                        "created": 1677610602,
                        "owned_by": f"llm-proxy-{provider}",
                        "root": model_id,
                        "parent": None,
                        "permission": [
                            {
                                "id": f"modelperm-{model_id}",
                                "object": "model_permission",
                                "created": 1677610602,
                                "allow_create_engine": False,
                                "allow_sampling": True,
                                "allow_logprobs": True,
                                "allow_search_indices": False,
                                "allow_view": True,
                                "allow_fine_tuning": False,
                                "organization": "*",
                                "group": None,
                                "is_blocking": False,
                            }
                        ],
                    }
                )
            return sorted(models, key=lambda x: str(x["id"]))
        except Exception as e:
            logger.error(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            raise ServiceError(
                message="è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥", error_code="MODEL_LIST_ERROR"
            )