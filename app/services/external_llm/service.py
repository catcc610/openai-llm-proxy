"""
External LLM Service - æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
"""

from __future__ import annotations
import json
import uuid
import asyncio
from typing import Any, AsyncGenerator, Dict, Union
import time
from fastapi import Request
from fastapi.responses import StreamingResponse
from litellm import acompletion
from litellm.utils import ModelResponse
from litellm.llms.anthropic.experimental_pass_through.adapters.transformation import (
    AnthropicAdapter,
)
from datetime import datetime

from app.services.base import BaseService, ServiceError
from app.services.external_llm.router import (
    get_provider_from_model,
    resolve_model,
)
from app.services.external_llm.provider_manager import ProviderManager
from logger.logger import get_logger
from config.config import get_external_llm_config

logger = get_logger(__name__)


class ExternalLLMService(BaseService):
    """å¤„ç†ä¸å¤–éƒ¨ LLM æä¾›å•†äº¤äº’çš„æ ¸å¿ƒæœåŠ¡"""

    def __init__(self) -> None:
        super().__init__("external_llm", get_external_llm_config())
        self.provider_manager = ProviderManager()
        logger.info("âœ… External LLM æœåŠ¡å·²æˆåŠŸåˆå§‹åŒ–")

    def get_models_info(self) -> list[dict[str, Any]]:
        """
        è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹çš„ä¿¡æ¯ï¼Œæ ¼å¼éµå¾ª OpenAI /models æ¥å£
        """
        config = get_external_llm_config()
        try:
            models = []
            for model_id, provider in config.get("provider_config", {}).items():
                models.append(
                    {
                        "id": model_id,
                        "object": "model",
                        "created": 1677610602,
                        "owned_by": f"llm-proxy-{provider}",
                        "root": model_id,
                        "parent": None,
                        "permission": [
                            {
                                "id": f"modelperm-{model_id}",
                                "object": "model_permission",
                                "created": 1677610602,
                                "allow_create_engine": False,
                                "allow_sampling": True,
                                "allow_logprobs": True,
                                "allow_search_indices": False,
                                "allow_view": True,
                                "allow_fine_tuning": False,
                                "organization": "*",
                                "group": None,
                                "is_blocking": False,
                            }
                        ],
                    }
                )
            return sorted(models, key=lambda x: str(x["id"]))
        except Exception as e:
            logger.error(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            raise ServiceError(
                message="è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥", error_code="MODEL_LIST_ERROR"
            )

    async def handle_chat_completion(
        self, request: Request
    ) -> Union[StreamingResponse, Dict[str, Any]]:
        """
        å¤„ç†èŠå¤©å®Œæˆè¯·æ±‚ï¼Œé€šè¿‡LiteLLMè½¬å‘åˆ°ç›¸åº”çš„LLMæä¾›å•†
        """
        request_id = (
            request.headers.get("X-Request-ID")
            or getattr(request.state, "request_id", None)
            or str(uuid.uuid4())
        )
        request.state.request_id = request_id

        try:
            t0 = time.time()
            payload = await request.json()
            t1 = time.time()

            model = payload.get("model")
            if not model:
                raise ServiceError(
                    message="è¯·æ±‚ä½“ä¸­ç¼ºå°‘ 'model' å­—æ®µ",
                    error_code="VALIDATION_ERROR",
                )

            stream = payload.get("stream", False)
            logger.info(
                f"ğŸ“¥ [{request_id}] æ”¶åˆ°èŠå¤©è¯·æ±‚: model={model}, stream={stream}"
            )

            litellm_params = self._prepare_litellm_params(payload, request_id)
            t2 = time.time()
            logger.debug(f"ğŸš€ [{request_id}] å¼€å§‹è°ƒç”¨LiteLLM...")

            response = await acompletion(**litellm_params)
            logger.debug(f"âœ… [{request_id}] LiteLLMè°ƒç”¨æˆåŠŸ")

            logger.info(
                f"â±ï¸ [{request_id}] è§£æè¯·æ±‚è€—æ—¶: {(t1 - t0):.3f} ç§’, å‚æ•°å‡†å¤‡è€—æ—¶: {(t2 - t1):.6f} ç§’"
            )

            if stream:
                return await self._handle_streaming_response(
                    response, model, request_id, t2
                )
            else:
                t4 = time.time()
                result = await self._convert_to_response_dict(
                    response, model, request_id
                )
                t5 = time.time()
                logger.info(
                    f"â±ï¸ [{request_id}] å“åº”è½¬æ¢è€—æ—¶: {(t5 - t4):.3f} ç§’, æ€»è€—æ—¶: {(t5 - t0):.3f} ç§’"
                )
                return result

        except json.JSONDecodeError as e:
            logger.error(f"âŒ [{request_id}] JSONè§£æå¤±è´¥: {e}")
            raise ServiceError(
                message="æ— æ•ˆçš„JSONæ ¼å¼", error_code="VALIDATION_JSON_ERROR"
            )
        except Exception as e:
            logger.error(
                "âŒ [{request_id}] èŠå¤©å®Œæˆå¤„ç†å¼‚å¸¸: {error_details}",
                request_id=request_id,
                error_details=e,
                exc_info=True,
            )
            raise ServiceError(message=str(e), error_code="CHAT_COMPLETION_ERROR")

    def _prepare_litellm_params(
        self, payload: Dict[str, Any], request_id: str
    ) -> Dict[str, Any]:
        """
        Prepares parameters for LiteLLM by delegating to the appropriate provider handler.
        """
        config = get_external_llm_config()
        model_name = payload.get("model")
        logger.info(
            f"ğŸ” [{request_id}] å‡†å¤‡LiteLLMå‚æ•°: model={model_name}, max_tokens={payload.get('max_tokens', 'None')}"
        )
        if not model_name or "messages" not in payload:
            raise ServiceError(
                message="Request body must contain 'model' and 'messages'",
                error_code="VALIDATION_REQUEST_ERROR",
            )

        # 1. Determine the provider and the actual model name for LiteLLM
        provider_name = get_provider_from_model(model_name, config)
        model_route = resolve_model(model_name, config)

        # Determine the base model name from the route config
        if isinstance(model_route, dict):
            # For complex routes (like Vertex), get the model name from the dict
            base_model_name = model_route.get("model") or model_name
        else:
            # For simple string routes, the route is the model name
            base_model_name = model_route

        # Always prepend the provider for LiteLLM, e.g., 'vertex_ai/gemini-pro'
        litellm_model = f"{provider_name}/{base_model_name}"
        logger.debug(f"Constructed final LiteLLM model ID: {litellm_model}")

        # 2. Get the specific provider handler from our factory
        provider_handler = self.provider_manager.get_provider(provider_name, config)

        # 3. Prepare a base payload for the handler
        base_payload = payload.copy()
        base_payload["model"] = litellm_model
        base_payload["drop_params"] = True

        # Add stream_options for streaming requests if not already present
        if base_payload.get("stream", False) and "stream_options" not in base_payload:
            base_payload["stream_options"] = {"include_usage": True}
            logger.debug(f"ğŸ”„ [{request_id}] æ·»åŠ æµå¼usageç»Ÿè®¡å‚æ•°: stream_options")

        # 4. Delegate the final parameter preparation to the handler
        final_params = provider_handler.prepare_litellm_params(
            base_payload, model_route
        )

        logger.debug(
            f"ğŸ” [{request_id}] Final LiteLLM params from '{provider_handler.__class__.__name__}': {final_params}"
        )
        return final_params

    async def _convert_to_response_dict(
        self, litellm_response: ModelResponse, original_model: str, request_id: str
    ) -> Dict[str, Any]:
        """å°†éæµå¼LiteLLMå“åº”è½¬æ¢ä¸ºOpenAIæ ¼å¼çš„å­—å…¸"""
        # Run synchronous, CPU-bound operation in a separate thread
        try:
            response_dict: Dict[str, Any] = await asyncio.to_thread(
                litellm_response.model_dump
            )
            response_dict["model"] = original_model

            # æ‰“å°å“åº”ID
            if response_id := response_dict.get("id"):
                logger.info(f"ğŸ†” [{request_id}] å“åº”ID: {response_id}")

            if usage := response_dict.get("usage"):
                logger.info(f"ğŸ“Š [{request_id}] Token usage: {usage}")

            return response_dict
        except Exception as e:
            logger.error(f"âŒ [{request_id}] å“åº”è½¬æ¢å¤±è´¥: {e}")
            raise ServiceError("å“åº”æ ¼å¼è½¬æ¢å¤±è´¥", "RESPONSE_CONVERSION_ERROR")

    async def _handle_streaming_response(
        self,
        litellm_stream: Any,
        original_model: str,
        request_id: str,
        start_time: float,
    ) -> StreamingResponse:
        """å¤„ç†æµå¼å“åº”"""

        async def generate_stream() -> AsyncGenerator[str, None]:
            final_usage = {}
            first_token_time = None
            response_id_logged = False  # æ ‡å¿—ä½ï¼Œç¡®ä¿å“åº”IDåªæ‰“å°ä¸€æ¬¡
            try:
                async for chunk in litellm_stream:
                    if first_token_time is None:
                        first_token_time = time.time()
                        elapsed = first_token_time - start_time
                        logger.info(
                            f"â±ï¸ [{request_id}] é¦– token å“åº”è€—æ—¶: {elapsed:.3f} ç§’ï¼Œ {datetime.now().strftime('%H:%M:%S.%f')[:-3]}"
                        )
                    chunk_dict = chunk.model_dump()
                    chunk_dict["model"] = original_model

                    # åœ¨ç¬¬ä¸€ä¸ªchunkæ‰“å°å“åº”ID
                    if not response_id_logged:
                        if response_id := chunk_dict.get("id"):
                            logger.info(
                                f"ğŸ†” [{request_id}] å“åº”ID (stream): {response_id}"
                            )
                            response_id_logged = True

                    if usage := chunk_dict.get("usage"):
                        final_usage = usage

                    yield f"data: {json.dumps(chunk_dict)}\n\n"
            except Exception as e:
                logger.error(f"âŒ [{request_id}] æµå¤„ç†å¼‚å¸¸: {e}")
                error_info = {
                    "error": {"message": f"æµå¤„ç†é”™è¯¯: {e}", "type": "STREAM_ERROR"}
                }
                yield f"data: {json.dumps(error_info)}\n\n"
            finally:
                if final_usage:
                    logger.info(
                        f"ğŸ“Š [{request_id}] Token usage (stream): {final_usage}"
                    )
                yield "data: [DONE]\n\n"

        return StreamingResponse(generate_stream(), media_type="text/event-stream")

    async def handle_anthropic_messages(
        self, request: Request
    ) -> Union[StreamingResponse, Dict[str, Any]]:
        """
        å¤„ç† Anthropic æ¶ˆæ¯æ ¼å¼è¯·æ±‚ï¼Œåˆ©ç”¨ LiteLLM çš„è½¬æ¢èƒ½åŠ›

        æµç¨‹ï¼š
        1. æ¥æ”¶ Anthropic æ ¼å¼çš„è¯·æ±‚
        2. ä½¿ç”¨ LiteLLM AnthropicAdapter è½¬æ¢ä¸º OpenAI æ ¼å¼
        3. è°ƒç”¨ acompletion ç”Ÿæˆå“åº”
        4. å°† OpenAI æ ¼å¼å“åº”è½¬æ¢å› Anthropic æ ¼å¼
        """
        request_id = (
            request.headers.get("X-Request-ID")
            or getattr(request.state, "request_id", None)
            or str(uuid.uuid4())
        )
        request.state.request_id = request_id

        try:
            t0 = time.time()
            payload = await request.json()
            t1 = time.time()

            model = payload.get("model")
            if not model:
                raise ServiceError(
                    message="è¯·æ±‚ä½“ä¸­ç¼ºå°‘ 'model' å­—æ®µ",
                    error_code="VALIDATION_ERROR",
                )

            stream = payload.get("stream", False)
            logger.info(
                f"ğŸ“¥ [{request_id}] æ”¶åˆ° Anthropic æ¶ˆæ¯è¯·æ±‚: model={model}, stream={stream}"
            )

            # 1. ä½¿ç”¨ LiteLLM AnthropicAdapter è½¬æ¢è¯·æ±‚æ ¼å¼
            anthropic_adapter = AnthropicAdapter()

            # æ³¨æ„ï¼štranslate_completion_input_params ä¼šä¿®æ”¹åŸå§‹å­—å…¸ï¼Œæ‰€ä»¥ä½¿ç”¨å‰¯æœ¬
            payload_for_conversion = payload.copy()
            openai_request = anthropic_adapter.translate_completion_input_params(
                payload_for_conversion
            )
            if not openai_request:
                raise ServiceError(
                    message="Anthropic è¯·æ±‚æ ¼å¼è½¬æ¢å¤±è´¥",
                    error_code="FORMAT_CONVERSION_ERROR",
                )

            # ChatCompletionRequest æ˜¯ Pydantic æ¨¡å‹ï¼Œè½¬æ¢ä¸ºå­—å…¸
            openai_payload = (
                openai_request.model_dump()
                if hasattr(openai_request, "model_dump")
                else dict(openai_request)
            )
            logger.debug(f"âœ… [{request_id}] Anthropic -> OpenAI æ ¼å¼è½¬æ¢æˆåŠŸ")

            # 2. å‡†å¤‡ LiteLLM å‚æ•°
            litellm_params = self._prepare_litellm_params(openai_payload, request_id)
            t2 = time.time()
            logger.debug(f"ğŸš€ [{request_id}] å¼€å§‹è°ƒç”¨ LiteLLM...")

            # 3. è°ƒç”¨ LiteLLM ç”Ÿæˆå“åº”
            response = await acompletion(**litellm_params)
            logger.debug(f"âœ… [{request_id}] LiteLLM è°ƒç”¨æˆåŠŸ")

            logger.info(
                f"â±ï¸ [{request_id}] è§£æè¯·æ±‚è€—æ—¶: {(t1 - t0):.3f} ç§’, å‚æ•°å‡†å¤‡è€—æ—¶: {(t2 - t1):.6f} ç§’"
            )

            # 4. å¤„ç†å“åº”è½¬æ¢
            if stream:
                return await self._handle_anthropic_streaming_response(
                    response, model, request_id, t2
                )
            else:
                t4 = time.time()
                result = await self._convert_to_anthropic_response_dict(
                    response, model, request_id
                )
                t5 = time.time()
                logger.info(
                    f"â±ï¸ [{request_id}] å“åº”è½¬æ¢è€—æ—¶: {(t5 - t4):.3f} ç§’, æ€»è€—æ—¶: {(t5 - t0):.3f} ç§’"
                )
                return result

        except json.JSONDecodeError as e:
            logger.error(f"âŒ [{request_id}] JSONè§£æå¤±è´¥: {e}")
            raise ServiceError(
                message="æ— æ•ˆçš„JSONæ ¼å¼", error_code="VALIDATION_JSON_ERROR"
            )
        except Exception as e:
            logger.error(
                "âŒ [{request_id}] Anthropic æ¶ˆæ¯å¤„ç†å¼‚å¸¸: {error_details}",
                request_id=request_id,
                error_details=e,
                exc_info=True,
            )
            raise ServiceError(message=str(e), error_code="ANTHROPIC_MESSAGES_ERROR")

    async def _convert_to_anthropic_response_dict(
        self, litellm_response: ModelResponse, original_model: str, request_id: str
    ) -> Dict[str, Any]:
        """å°† LiteLLM OpenAI æ ¼å¼å“åº”è½¬æ¢ä¸º Anthropic æ ¼å¼"""
        try:
            anthropic_adapter = AnthropicAdapter()

            # ä½¿ç”¨ LiteLLM é€‚é…å™¨è½¬æ¢å“åº”
            anthropic_response = anthropic_adapter.translate_completion_output_params(
                litellm_response
            )
            if not anthropic_response:
                logger.error(f"âŒ [{request_id}] Anthropic å“åº”è½¬æ¢è¿”å› None")
                raise ServiceError(
                    "Anthropic response conversion failed.",
                    "ANTHROPIC_RESPONSE_CONVERSION_ERROR",
                )

            # æ£€æŸ¥è¿”å›çš„æ˜¯ Pydantic æ¨¡å‹è¿˜æ˜¯å­—å…¸
            if hasattr(anthropic_response, "model_dump"):
                # Pydantic æ¨¡å‹ï¼Œè°ƒç”¨ model_dump()
                response_dict: Dict[str, Any] = anthropic_response.model_dump()
            elif hasattr(anthropic_response, "dict"):
                # Pydantic æ¨¡å‹ï¼ˆæ—§ç‰ˆæœ¬ï¼‰ï¼Œè°ƒç”¨ dict()
                response_dict = anthropic_response.dict()
            else:
                # å·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
                if isinstance(anthropic_response, dict):
                    response_dict = anthropic_response
                else:
                    response_dict = dict(anthropic_response)

            response_dict["model"] = original_model

            # æ‰“å°å“åº”ID
            if response_id := response_dict.get("id"):
                logger.info(f"ğŸ†” [{request_id}] Anthropic å“åº”ID: {response_id}")

            if usage := response_dict.get("usage"):
                logger.info(f"ğŸ“Š [{request_id}] Token usage: {usage}")

            logger.debug(f"âœ… [{request_id}] OpenAI -> Anthropic æ ¼å¼è½¬æ¢æˆåŠŸ")

            return response_dict

        except Exception as e:
            logger.error(
                f"âŒ [{request_id}] Anthropic å“åº”è½¬æ¢å¤±è´¥: {e}", exc_info=True
            )
            raise ServiceError(
                "Anthropic å“åº”æ ¼å¼è½¬æ¢å¤±è´¥", "ANTHROPIC_RESPONSE_CONVERSION_ERROR"
            )

    async def _handle_anthropic_streaming_response(
        self,
        litellm_stream: Any,
        original_model: str,
        request_id: str,
        start_time: float,
    ) -> StreamingResponse:
        """å¤„ç† Anthropic æ ¼å¼çš„æµå¼å“åº”"""

        async def openai_usage_logging_stream(stream: Any) -> AsyncGenerator[Any, None]:
            """ä»£ç†æµæ—¶è®°å½• OpenAI çš„ usageï¼Œå¹¶é€ä¼ æ•°æ®å—"""
            final_usage = None
            async for chunk in stream:
                if hasattr(chunk, "usage") and chunk.usage:
                    final_usage = chunk.usage
                yield chunk
            if final_usage:
                try:
                    usage_dict = (
                        final_usage.model_dump()
                        if hasattr(final_usage, "model_dump")
                        else dict(final_usage)
                    )
                    logger.info(
                        f"ğŸ“Š [{request_id}] OpenAI format usage (from stream): {json.dumps(usage_dict)}"
                    )
                except Exception as e:
                    logger.warning(f"âš ï¸ [{request_id}] æ— æ³•åºåˆ—åŒ– usage ä¿¡æ¯: {e}")

        async def generate_anthropic_stream() -> AsyncGenerator[str, None]:
            first_token_time = None

            try:
                # ä½¿ç”¨åŒ…è£…å™¨è®°å½• OpenAI usage
                logged_litellm_stream = openai_usage_logging_stream(litellm_stream)

                anthropic_adapter = AnthropicAdapter()

                # ä½¿ç”¨ LiteLLM é€‚é…å™¨çš„æµå¼è½¬æ¢
                anthropic_stream = (
                    anthropic_adapter.translate_completion_output_params_streaming(
                        logged_litellm_stream, original_model
                    )
                )

                if not anthropic_stream:
                    logger.error(f"âŒ [{request_id}] Anthropic é€‚é…å™¨æœªèƒ½åˆ›å»ºæµã€‚")
                    raise ServiceError(
                        "Anthropic adapter failed to create a stream.",
                        "ANTHROPIC_STREAM_CREATION_ERROR",
                    )

                async for chunk_data in anthropic_stream:
                    if first_token_time is None:
                        first_token_time = time.time()
                        elapsed = first_token_time - start_time
                        logger.info(
                            f"â±ï¸ [{request_id}] é¦– token å“åº”è€—æ—¶: {elapsed:.3f} ç§’ï¼Œ {datetime.now().strftime('%H:%M:%S.%f')[:-3]}"
                        )

                    # LiteLLM é€‚é…å™¨å¯èƒ½è¿”å›å­—èŠ‚æµæˆ–å­—ç¬¦ä¸²
                    if isinstance(chunk_data, bytes):
                        yield chunk_data.decode("utf-8")
                    elif isinstance(chunk_data, str):
                        yield chunk_data
                    else:
                        yield f"data: {json.dumps(chunk_data)}\n\n"

                logger.debug(f"âœ… [{request_id}] Anthropic æµå¼è½¬æ¢å®Œæˆ")

            except Exception as e:
                logger.error(
                    f"âŒ [{request_id}] Anthropic æµå¤„ç†å¼‚å¸¸: {e}", exc_info=True
                )
                error_info = {
                    "error": {
                        "message": f"Anthropic æµå¤„ç†é”™è¯¯: {e}",
                        "type": "ANTHROPIC_STREAM_ERROR",
                    }
                }
                yield f"data: {json.dumps(error_info)}\n\n"
            finally:
                yield "data: [DONE]\n\n"

        return StreamingResponse(
            generate_anthropic_stream(), media_type="text/event-stream"
        )

    def get_all_provider_stats(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰æä¾›å•†çš„çŠ¶æ€"""
        config = get_external_llm_config()
        return self.provider_manager.get_all_provider_stats(config)

    async def get_models(self) -> list[dict[str, Any]]:
        """
        è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹, æ ¼å¼éµå¾ªOpenAIè§„èŒƒã€‚
        """
        config = get_external_llm_config()
        try:
            models = []
            for model_id, provider in config.get("provider_config", {}).items():
                models.append(
                    {
                        "id": model_id,
                        "object": "model",
                        "created": 1677610602,
                        "owned_by": f"llm-proxy-{provider}",
                        "root": model_id,
                        "parent": None,
                        "permission": [
                            {
                                "id": f"modelperm-{model_id}",
                                "object": "model_permission",
                                "created": 1677610602,
                                "allow_create_engine": False,
                                "allow_sampling": True,
                                "allow_logprobs": True,
                                "allow_search_indices": False,
                                "allow_view": True,
                                "allow_fine_tuning": False,
                                "organization": "*",
                                "group": None,
                                "is_blocking": False,
                            }
                        ],
                    }
                )
            return sorted(models, key=lambda x: str(x["id"]))
        except Exception as e:
            logger.error(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            raise ServiceError(
                message="è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥", error_code="MODEL_LIST_ERROR"
            )
