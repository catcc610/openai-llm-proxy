"""
LLM Proxy Service - FastAPI Main Application
"""

# æ ‡å‡†åº“å¯¼å…¥
import json
import logging
import os
import time
import uuid
from typing import Dict, Any, Union

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse

# LiteLLMå¯¼å…¥
from litellm import acompletion

# æœ¬åœ°åº”ç”¨å¯¼å…¥
from app.config import get_config, reload_config
from app.router import resolve_model, get_provider_from_model
from app.errors import setup_error_handlers, handle_streaming_error
from app.middleware import RequestIDMiddleware, LoggingMiddleware
from app.models import (
    ChatCompletionRequest,
    ChatCompletionResponse,
    MessageRole,
    ChatMessage,
    Choice,
    Usage,
    ChoiceFinishReason,
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# è®¾ç½®LiteLLMæ—¥å¿—çº§åˆ«ï¼ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
os.environ["LITELLM_LOG"] = "INFO"  # æ›´æ–°çš„æ—¥å¿—è®¾ç½®æ–¹å¼

# æ³¨æ„ï¼šAPIå¯†é’¥ç°åœ¨é€šè¿‡config.pyè‡ªåŠ¨è®¾ç½®ï¼Œæ— éœ€æ‰‹åŠ¨å¤„ç†

# åˆ›å»ºFastAPIåº”ç”¨å®ä¾‹
app = FastAPI(
    title="LLM Proxy Service",
    description="åŸºäºLiteLLMçš„å¤šå‚å•†LLMä»£ç†æœåŠ¡",
    version="0.1.0",
    docs_url="/docs",
    redoc_url="/redoc",
)

# è®¾ç½®é”™è¯¯å¤„ç†å™¨
setup_error_handlers(app)

# æ·»åŠ ä¸­é—´ä»¶ï¼ˆæ³¨æ„é¡ºåºï¼šæœ€åæ·»åŠ çš„æœ€å…ˆæ‰§è¡Œï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ·»åŠ è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶
app.add_middleware(LoggingMiddleware, log_body=False)

# æ·»åŠ è¯·æ±‚IDä¸­é—´ä»¶ï¼ˆæœ€å…ˆæ‰§è¡Œï¼‰
app.add_middleware(RequestIDMiddleware)


@app.get("/")
async def root() -> Dict[str, str]:
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {
        "message": "LLM Proxy Service is running",
        "version": "0.1.0",
        "status": "healthy",
    }


@app.get("/health")
async def health_check() -> Dict[str, str]:
    """è¯¦ç»†å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "service": "llm-proxy", "version": "0.1.0"}


@app.get("/config")
async def get_config_info() -> Dict[str, Any]:
    """è·å–é…ç½®ä¿¡æ¯ï¼ˆä¸åŒ…å«æ•æ„Ÿæ•°æ®ï¼‰"""
    try:
        config = get_config()
        return {
            "server": config.server.model_dump(),
            "proxy": config.proxy.model_dump(),
            "available_models": list(config.model_routes.keys()),
            "security_enabled": len(config.security.api_keys) > 0,
        }
    except Exception as e:
        return {"error": f"é…ç½®è·å–å¤±è´¥: {str(e)}"}


@app.post("/config/reload")
async def reload_config_endpoint() -> Dict[str, str]:
    """é‡æ–°åŠ è½½é…ç½®"""
    try:
        reload_config()
        return {"message": "é…ç½®é‡æ–°åŠ è½½æˆåŠŸ"}
    except Exception as e:
        return {"error": f"é…ç½®é‡æ–°åŠ è½½å¤±è´¥: {str(e)}"}


# ================================
# OpenAI å…¼å®¹çš„ API ç«¯ç‚¹
# ================================


@app.post("/v1/chat/completions", response_model=None)
async def chat_completions(
    request: ChatCompletionRequest, http_request: Request
) -> Union[ChatCompletionResponse, StreamingResponse]:
    """
    OpenAI å…¼å®¹çš„èŠå¤©å®Œæˆç«¯ç‚¹
    é€šè¿‡LiteLLMè½¬å‘è¯·æ±‚åˆ°ç›¸åº”çš„LLMæä¾›å•†
    """
    request_id = getattr(http_request.state, "request_id", str(uuid.uuid4()))

    try:
        # è®°å½•è¯·æ±‚ä¿¡æ¯
        logger.info(f"ğŸ“¥ [{request_id}] èŠå¤©å®Œæˆè¯·æ±‚:")
        logger.info(f"   æ¨¡å‹: {request.model}")
        logger.info(f"   æ¶ˆæ¯æ•°é‡: {len(request.messages)}")
        logger.info(f"   æ¸©åº¦: {request.temperature}")
        logger.info(f"   æµå¼: {request.stream}")

        # ä½¿ç”¨è·¯ç”±å™¨è§£ææ¨¡å‹
        litellm_model = resolve_model(request.model)
        provider = get_provider_from_model(request.model)
        logger.info(
            f"ğŸ”„ [{request_id}] æ¨¡å‹è·¯ç”±: {request.model} -> {litellm_model} (æä¾›å•†: {provider})"
        )

        # ä½¿ç”¨è¯·æ±‚æ¨¡å‹çš„æ–¹æ³•è‡ªåŠ¨è½¬æ¢ä¸ºLiteLLMå‚æ•°
        litellm_params = request.to_litellm_params(litellm_model)

        logger.debug(
            f"ğŸš€ [{request_id}] è°ƒç”¨LiteLLMï¼Œå‚æ•°: {json.dumps(litellm_params, indent=2, ensure_ascii=False)}"
        )

        # è°ƒç”¨LiteLLM (å¼‚æ­¥è°ƒç”¨)
        response = await acompletion(**litellm_params)

        if request.stream:
            # æµå¼å“åº”å¤„ç†
            return await handle_litellm_streaming_response(
                response, request.model, request_id
            )
        else:
            # éæµå¼å“åº”å¤„ç†
            return convert_litellm_response(response, request.model)

    except Exception as e:
        # ä½¿ç”¨æˆ‘ä»¬çš„é”™è¯¯å¤„ç†ç³»ç»Ÿ
        from app.errors import handle_proxy_error

        return await handle_proxy_error(http_request, e)


def convert_litellm_response(
    litellm_response: Any, original_model: str
) -> ChatCompletionResponse:
    """å°†LiteLLMå“åº”è½¬æ¢ä¸ºæˆ‘ä»¬çš„Pydanticæ¨¡å‹"""
    try:
        # LiteLLMè¿”å›çš„å“åº”å·²ç»æ˜¯OpenAIæ ¼å¼ï¼Œç›´æ¥è½¬æ¢
        response_dict = (
            litellm_response.model_dump()
            if hasattr(litellm_response, "model_dump")
            else dict(litellm_response)
        )

        # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„å­—æ®µéƒ½å­˜åœ¨
        if "id" not in response_dict:
            response_dict["id"] = f"chatcmpl-{uuid.uuid4().hex[:8]}"
        if "created" not in response_dict:
            response_dict["created"] = int(time.time())
        if "object" not in response_dict:
            response_dict["object"] = "chat.completion"

        # ç¡®ä¿è¿”å›åŸå§‹è¯·æ±‚çš„æ¨¡å‹åç§°ï¼Œè€Œä¸æ˜¯LiteLLMçš„å†…éƒ¨æ¨¡å‹å
        response_dict["model"] = original_model

        return ChatCompletionResponse(**response_dict)
    except Exception as e:
        print(f"âŒ å“åº”è½¬æ¢é”™è¯¯: {e}")
        # åˆ›å»ºå¤‡ç”¨å“åº”
        return ChatCompletionResponse(
            id=f"chatcmpl-{uuid.uuid4().hex[:8]}",
            object="chat.completion",
            created=int(time.time()),
            model=original_model,
            choices=[
                Choice(
                    index=0,
                    message=ChatMessage(
                        role=MessageRole.ASSISTANT, content=str(litellm_response)
                    ),
                    finish_reason=ChoiceFinishReason.STOP,
                )
            ],
            usage=Usage(prompt_tokens=0, completion_tokens=0, total_tokens=0),
        )


async def handle_litellm_streaming_response(
    litellm_stream: Any, original_model: str, request_id: str | None = None
) -> StreamingResponse:
    """å¤„ç†LiteLLMçš„æµå¼å“åº”"""

    async def generate_stream() -> Any:
        try:
            # å¼‚æ­¥è¿­ä»£æµå¼å“åº”
            async for chunk in litellm_stream:
                # LiteLLMè¿”å›çš„æµå¼å“åº”å·²ç»æ˜¯OpenAIæ ¼å¼
                chunk_dict = (
                    chunk.model_dump() if hasattr(chunk, "model_dump") else dict(chunk)
                )

                # ç¡®ä¿chunkæœ‰æ­£ç¡®çš„æ ¼å¼
                if "object" not in chunk_dict:
                    chunk_dict["object"] = "chat.completion.chunk"
                if "id" not in chunk_dict:
                    chunk_dict["id"] = f"chatcmpl-{request_id or uuid.uuid4().hex[:8]}"
                if "created" not in chunk_dict:
                    chunk_dict["created"] = int(time.time())

                # ç¡®ä¿è¿”å›åŸå§‹è¯·æ±‚çš„æ¨¡å‹åç§°
                chunk_dict["model"] = original_model

                yield f"data: {json.dumps(chunk_dict, ensure_ascii=False)}\n\n"

        except Exception as e:
            logger.error(f"âŒ [{request_id}] æµå¼å“åº”å¤„ç†é”™è¯¯: {e}")
            # å‘é€é”™è¯¯å“åº”å—
            error_data = await handle_streaming_error(e, original_model, request_id)
            yield error_data
        finally:
            yield "data: [DONE]\n\n"

    return StreamingResponse(
        generate_stream(),
        media_type="text/plain",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # ç¦ç”¨nginxç¼“å†²
        },
    )
