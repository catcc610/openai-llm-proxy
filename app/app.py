"""
LLM Proxy Service - FastAPI Main Application
"""

# æ ‡å‡†åº“å¯¼å…¥
import logging
import uuid
from typing import Dict, Any, Union

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse

# LiteLLMå¯¼å…¥
from litellm import acompletion
from litellm.utils import ModelResponse, CustomStreamWrapper

# æœ¬åœ°åº”ç”¨å¯¼å…¥
from app.config import get_config
from app.router import resolve_model
from app.errors import setup_error_handlers, handle_proxy_error
from app.middleware import RequestIDMiddleware, LoggingMiddleware
from app.models import ChatCompletionRequest
from app.response_formatter import (
    convert_litellm_response_to_dict,
    handle_litellm_streaming_response,
)


def create_app() -> FastAPI:
    """åˆ›å»ºå¹¶é…ç½®FastAPIåº”ç”¨å®ä¾‹"""

    # è·å–æ—¥å¿—è®°å½•å™¨
    logger = logging.getLogger(__name__)
    logger.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ–LLM Proxy Service...")

    # åˆ›å»ºFastAPIåº”ç”¨å®ä¾‹
    app = FastAPI(
        title="LLM Proxy Service",
        description="åŸºäºLiteLLMçš„å¤šå‚å•†LLMä»£ç†æœåŠ¡",
        version="0.1.0",
        docs_url="/docs",
        redoc_url="/redoc",
    )

    # è®¾ç½®é”™è¯¯å¤„ç†å™¨
    setup_error_handlers(app)

    # æ·»åŠ ä¸­é—´ä»¶ï¼ˆæ³¨æ„é¡ºåºï¼šæœ€åæ·»åŠ çš„æœ€å…ˆæ‰§è¡Œï¼‰
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥é™åˆ¶å…·ä½“åŸŸå
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # æ·»åŠ è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶
    app.add_middleware(LoggingMiddleware, log_body=False)

    # æ·»åŠ è¯·æ±‚IDä¸­é—´ä»¶ï¼ˆæœ€å…ˆæ‰§è¡Œï¼‰
    app.add_middleware(RequestIDMiddleware)

    logger.info("âœ… FastAPIåº”ç”¨åˆå§‹åŒ–å®Œæˆ")
    return app


# åˆ›å»ºåº”ç”¨å®ä¾‹
app = create_app()

# è·å–æ—¥å¿—è®°å½•å™¨ï¼ˆåœ¨appåˆ›å»ºåï¼‰
logger = logging.getLogger(__name__)


@app.get("/")
async def root() -> Dict[str, str]:
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {
        "message": "LLM Proxy Service is running",
        "version": "0.1.0",
        "status": "healthy",
    }


@app.get("/health")
async def health_check() -> Dict[str, str]:
    """è¯¦ç»†å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "service": "llm-proxy", "version": "0.1.0"}


@app.get("/config")
async def get_config_info() -> Dict[str, Any]:
    """è·å–é…ç½®ä¿¡æ¯ï¼ˆä¸åŒ…å«æ•æ„Ÿæ•°æ®ï¼‰"""
    try:
        config = get_config()
        return {
            "server": config.server.model_dump(),
            "proxy": config.proxy.model_dump(),
            "available_models": list(config.model_routes.keys()),
            "security_enabled": len(config.security.api_keys) > 0,
        }
    except Exception as e:
        return {"error": f"é…ç½®è·å–å¤±è´¥: {str(e)}"}


# ================================
# OpenAI å…¼å®¹çš„ API ç«¯ç‚¹
# ================================


@app.post("/v1/chat/completions", response_model=None, status_code=200)
async def chat_completions(
    request: ChatCompletionRequest, http_request: Request
) -> Union[Dict[str, Any], StreamingResponse]:
    """
    OpenAI å…¼å®¹çš„èŠå¤©å®Œæˆç«¯ç‚¹ - æ”¯æŒå¤šæ¨¡æ€ã€æµå¼å’Œå·¥å…·è°ƒç”¨ã€‚
    é€šè¿‡LiteLLMå°†è¯·æ±‚è·¯ç”±å¹¶è½¬å‘åˆ°ç›¸åº”çš„LLMæä¾›å•†ã€‚
    """
    request_id = getattr(http_request.state, "request_id", str(uuid.uuid4()))

    try:
        logger.info(
            f"ğŸ“¥ [{request_id}] æ¥æ”¶åˆ°èŠå¤©è¯·æ±‚: model={request.model}, stream={request.stream}"
        )
        if any(isinstance(msg.content, list) for msg in request.messages):
            logger.info(f"ğŸ–¼ï¸ [{request_id}] è¯·æ±‚åŒ…å«å¤šæ¨¡æ€å†…å®¹")

        litellm_model = resolve_model(request.model)
        logger.info(f"ğŸ”„ [{request_id}] æ¨¡å‹è·¯ç”±: {request.model} -> {litellm_model}")

        litellm_params = request.to_litellm_params(litellm_model=litellm_model)

        # æ•æ„Ÿä¿¡æ¯ä¸åº”è¢«è®°å½•åˆ°æ—¥å¿—
        # logger.debug(f"ğŸš€ [{request_id}] è°ƒç”¨LiteLLMå‚æ•°: {json.dumps(litellm_params, indent=2, ensure_ascii=False)}")

        response: Union[ModelResponse, CustomStreamWrapper] = await acompletion(
            **litellm_params
        )

        if request.stream:
            # ç¡®ä¿è¿”å›çš„æ˜¯æµå¼åŒ…è£…å™¨
            if not isinstance(response, CustomStreamWrapper):
                # è¿™é‡Œå¯ä»¥å¼•å‘ä¸€ä¸ªè‡ªå®šä¹‰çš„å†…éƒ¨é”™è¯¯
                raise TypeError("é¢„æœŸå¾—åˆ°æµå¼å“åº”ï¼Œä½†æ”¶åˆ°äº†éæµå¼å“åº”")
            logger.info(f"â¡ï¸ [{request_id}] è½¬å‘æµå¼å“åº”")
            return await handle_litellm_streaming_response(
                response, request.model, request_id
            )
        else:
            # ç¡®ä¿è¿”å›çš„æ˜¯æ¨¡å‹å“åº”å¯¹è±¡
            if not isinstance(response, ModelResponse):
                raise TypeError("é¢„æœŸå¾—åˆ°éæµå¼å“åº”ï¼Œä½†æ”¶åˆ°äº†æµå¼å“åº”")
            logger.info(f"â¬…ï¸ [{request_id}] è½¬å‘éæµå¼å“åº”")
            return convert_litellm_response_to_dict(response, request.model)

    except Exception as e:
        return await handle_proxy_error(http_request, e)
