"""
LLM Proxy - Response Formatting Utilities

å°†LiteLLMçš„å“åº”å¯¹è±¡è½¬æ¢ä¸ºOpenAIå…¼å®¹çš„å­—å…¸æˆ–æµå¼æ•°æ®å—ã€‚
"""

import json
import logging
import time
import uuid
from typing import Any, AsyncGenerator

from fastapi.responses import StreamingResponse
from litellm.utils import CustomStreamWrapper, ModelResponse

logger = logging.getLogger(__name__)


def convert_litellm_response_to_dict(
    litellm_response: ModelResponse, original_model: str
) -> dict[str, Any]:
    """å°†LiteLLMçš„éæµå¼å“åº”è½¬æ¢ä¸ºä¸OpenAIå…¼å®¹çš„å­—å…¸æ ¼å¼"""
    try:
        response_dict: dict[str, Any] = litellm_response.model_dump()
        response_dict["model"] = original_model
        return response_dict
    except Exception as e:
        logger.error(f"âŒ LiteLLMå“åº”è½¬æ¢ä¸ºå­—å…¸æ—¶å‡ºé”™: {e}", exc_info=True)
        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªå¤‡ç”¨çš„é”™è¯¯å“åº”
        return {
            "id": f"chatcmpl-error-{uuid.uuid4().hex}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": original_model,
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": f"å¤„ç†åç«¯å“åº”æ—¶å‘ç”Ÿå†…éƒ¨é”™è¯¯: {str(e)}",
                    },
                    "finish_reason": "error",
                }
            ],
            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
        }


async def handle_litellm_streaming_response(
    litellm_stream: CustomStreamWrapper, original_model: str, request_id: str
) -> StreamingResponse:
    """å¤„ç†LiteLLMçš„æµå¼å“åº”ï¼Œç”Ÿæˆå…¼å®¹OpenAIçš„SSEæµ"""

    async def generate_stream() -> AsyncGenerator[str, None]:
        try:
            logger.info(f"ğŸŒŠ [{request_id}] å¼€å§‹å¤„ç†æµå¼å“åº”...")
            async for chunk in litellm_stream:
                chunk_dict = chunk.model_dump()
                chunk_dict["model"] = original_model
                yield f"data: {json.dumps(chunk_dict)}\n\n"
            logger.info(f"âœ… [{request_id}] æµå¼å“åº”å¤„ç†å®Œæˆ")
        except Exception as e:
            logger.error(
                f"âŒ [{request_id}] æµå¼å“åº”å¤„ç†ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True
            )
            error_chunk = {
                "id": f"chatcmpl-error-{request_id}",
                "object": "chat.completion.chunk",
                "created": int(time.time()),
                "model": original_model,
                "choices": [
                    {
                        "index": 0,
                        "delta": {
                            "role": "assistant",
                            "content": f"æµå¼å¤„ç†é”™è¯¯: {str(e)}",
                        },
                        "finish_reason": "error",
                    }
                ],
            }
            yield f"data: {json.dumps(error_chunk)}\n\n"
        finally:
            yield "data: [DONE]\n\n"

    return StreamingResponse(
        generate_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Nginx-specific header for disabling buffering
        },
    )
