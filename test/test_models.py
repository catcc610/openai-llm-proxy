#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM ä»£ç†æ¨¡å‹æµ‹è¯•è„šæœ¬
æµ‹è¯• config.yaml ä¸­é…ç½®çš„æ‰€æœ‰æ¨¡å‹çš„å›å¤æ•ˆæœ, å¹¶æ”¯æŒå¤šæ¨¡æ€ã€å·¥å…·è°ƒç”¨å’Œæµå¼æµ‹è¯•ã€‚
"""

import yaml
import asyncio
from openai import AsyncOpenAI
import time
from typing import Dict, List
import json


class ModelTester:
    def __init__(self, config_path: str = "../config/external_llm/external_llm.yaml"):
        """åˆå§‹åŒ–æ¨¡å‹æµ‹è¯•å™¨"""
        # åŠ è½½é…ç½®æ–‡ä»¶
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # è·å–æœåŠ¡å™¨é…ç½®
        server_config = self.config.get('server', {})
        host = server_config.get('host', 'localhost')
        port = 9000
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼Œè¿æ¥åˆ°æœ¬åœ°ä»£ç†æœåŠ¡
        self.client = AsyncOpenAI(
            base_url=f"http://{host}:{port}/v1",
            api_key="test-key"  # ä»£ç†æœåŠ¡çš„APIå¯†é’¥ï¼Œå¦‚æœä¸éœ€è¦è®¤è¯å¯ä»¥éšæ„è®¾ç½®
        )
        
        # è·å–æ¨¡å‹åˆ—è¡¨
        self.models = list(self.config.get('provider_config', {}).keys())
        
        # ä¸ºä¸åŒæµ‹è¯•ç­›é€‰æ¨¡å‹
        self.multimodal_models = [m for m in self.models if 'gemini' in m]
        self.tool_call_models = [m for m in self.models if 'sonnet' in m or 'deepseek' in m]
        self.streaming_test_models = [m for m in self.models if 'flash' in m or 'sonnet' in m or 'deepseek' in m][:2] # é€‰æ‹©æœ€å¤š2ä¸ªæ¨¡å‹è¿›è¡Œæµå¼æµ‹è¯•

        # æµ‹è¯•é—®é¢˜
        self.test_questions = [
            "è¯·ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ã€‚"
        ]

        # å¤šæ¨¡æ€æµ‹è¯•
        self.multimodal_question = {
            "text": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿè¯·ç”¨ä¸­æ–‡æè¿°ã€‚",
            "image_url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
        }

        # å·¥å…·è°ƒç”¨æµ‹è¯•
        self.tool_call_question = "æ—§é‡‘å±±ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
        self.tools = [
            {
                "type": "function",
                "function": {
                    "name": "get_current_weather",
                    "description": "Get the current weather in a given location",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "location": {
                                "type": "string",
                                "description": "The city and state, e.g. San Francisco, CA",
                            },
                            "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                        },
                        "required": ["location"],
                    },
                },
            }
        ]

    async def test_single_model(self, model_name: str, question: str) -> Dict:
        """æµ‹è¯•å•ä¸ªæ¨¡å‹ï¼ˆéæµå¼ï¼‰"""
        print(f"æ­£åœ¨æµ‹è¯•æ¨¡å‹: {model_name}")
        
        try:
            start_time = time.time()
            
            response = await self.client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "user", "content": question}
                ],
                max_tokens=4096,
                temperature=0.7
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            result = {
                "id": response.id,
                "model": model_name,
                "question": question,
                "response": response.choices[0].message.content,
                "response_time": round(response_time, 2),
                "success": True,
                "error": None,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                    "completion_tokens": response.usage.completion_tokens if response.usage else 0,
                    "total_tokens": response.usage.total_tokens if response.usage else 0
                }
            }
            
            print(f"âœ… {model_name} - å“åº”æ—¶é—´: {response_time:.2f}ç§’")
            
        except Exception as e:
            result = {
                "id": None,
                "model": model_name,
                "question": question,
                "response": None,
                "response_time": 0,
                "success": False,
                "error": str(e),
                "usage": None
            }
            print(f"âŒ {model_name} - é”™è¯¯: {str(e)}")
        
        return result

    async def test_streaming_single_model(self, model_name: str, question: str) -> Dict:
        """æµ‹è¯•å•ä¸ªæ¨¡å‹çš„æµå¼å“åº”"""
        print(f"æ­£åœ¨æµ‹è¯•æµå¼æ¨¡å‹: {model_name}")
        
        start_time = time.time()
        time_to_first_token = None
        response_id = None
        full_response = ""
        
        try:
            stream = await self.client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": question}],
                max_tokens=4096,
                temperature=0.7,
                stream=True
            )
            
            async for chunk in stream:
                if time_to_first_token is None and chunk.choices and chunk.choices[0].delta.content:
                    time_to_first_token = time.time() - start_time
                
                if not response_id:
                    response_id = chunk.id

                if chunk.choices and chunk.choices[0].delta.content is not None:
                    full_response += chunk.choices[0].delta.content
            
            end_time = time.time()
            response_time = end_time - start_time
            
            result = {
                "id": response_id,
                "model": model_name,
                "question": question,
                "response": full_response,
                "response_time": round(response_time, 2),
                "time_to_first_token": round(time_to_first_token, 2) if time_to_first_token else 0,
                "success": True,
                "error": None,
                "usage": None,  # Usage is not easily available in streaming mode with proxy
                "type": "streaming"
            }
            print(f"âœ… {model_name} (æµå¼) - TTFT: {result['time_to_first_token']:.2f}s, æ€»æ—¶é—´: {result['response_time']:.2f}s")
        except Exception as e:
            result = {
                "id": response_id,
                "model": model_name,
                "question": question,
                "response": None,
                "response_time": 0,
                "time_to_first_token": 0,
                "success": False,
                "error": str(e),
                "usage": None,
                "type": "streaming"
            }
            print(f"âŒ {model_name} (æµå¼) - é”™è¯¯: {str(e)}")
        
        return result

    async def test_multimodal_single_model(self, model_name: str) -> Dict:
        """æµ‹è¯•å•ä¸ªæ¨¡å‹çš„å¤šæ¨¡æ€èƒ½åŠ›"""
        print(f"æ­£åœ¨æµ‹è¯•å¤šæ¨¡æ€æ¨¡å‹: {model_name}")
        question = self.multimodal_question["text"]
        
        try:
            start_time = time.time()
            
            response = await self.client.chat.completions.create(
                model=model_name,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": question},
                            {
                                "type": "image_url",
                                "image_url": {"url": self.multimodal_question["image_url"]},
                            },
                        ],
                    }
                ],
                max_tokens=4096,
                temperature=0.7
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            result = {
                "id": response.id,
                "model": model_name,
                "question": question,
                "image_url": self.multimodal_question["image_url"],
                "response": response.choices[0].message.content,
                "response_time": round(response_time, 2),
                "success": True,
                "error": None,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                    "completion_tokens": response.usage.completion_tokens if response.usage else 0,
                    "total_tokens": response.usage.total_tokens if response.usage else 0
                }
            }
            
            print(f"âœ… {model_name} - å“åº”æ—¶é—´: {response_time:.2f}ç§’")
            
        except Exception as e:
            result = {
                "id": None,
                "model": model_name,
                "question": question,
                "image_url": self.multimodal_question["image_url"],
                "response": None,
                "response_time": 0,
                "success": False,
                "error": str(e),
                "usage": None
            }
            print(f"âŒ {model_name} - é”™è¯¯: {str(e)}")
        
        return result

    async def test_tool_call_single_model(self, model_name: str) -> Dict:
        """æµ‹è¯•å•ä¸ªæ¨¡å‹çš„å·¥å…·è°ƒç”¨èƒ½åŠ›"""
        print(f"æ­£åœ¨æµ‹è¯•å·¥å…·è°ƒç”¨æ¨¡å‹: {model_name}")
        question = self.tool_call_question
        
        try:
            start_time = time.time()
            
            response = await self.client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": question}],
                tools=self.tools,
                tool_choice="auto",
                max_tokens=4096,
                temperature=0.7
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            response_message = response.choices[0].message
            tool_calls = response_message.tool_calls
            
            result = {
                "id": response.id,
                "model": model_name,
                "question": question,
                "response_time": round(response_time, 2),
                "success": True,
                "error": None,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                    "completion_tokens": response.usage.completion_tokens if response.usage else 0,
                    "total_tokens": response.usage.total_tokens if response.usage else 0
                },
                "tool_calls": None
            }
            
            if tool_calls:
                result["response"] = f"è¯·æ±‚å·¥å…·è°ƒç”¨: {tool_calls[0].function.name}"
                result["tool_calls"] = [{"name": tc.function.name, "arguments": tc.function.arguments} for tc in tool_calls]
                print(f"âœ… {model_name} - å·¥å…·è°ƒç”¨æˆåŠŸ in {response_time:.2f}s")
            else:
                result["response"] = response_message.content
                print(f"âš ï¸ {model_name} - æœªè¿”å›å·¥å…·è°ƒç”¨ in {response_time:.2f}s")

        except Exception as e:
            result = {
                "id": None,
                "model": model_name,
                "question": question,
                "response": None,
                "response_time": 0,
                "success": False,
                "error": str(e),
                "usage": None,
                "tool_calls": None
            }
            print(f"âŒ {model_name} - é”™è¯¯: {str(e)}")
        
        return result

    async def _run_tests(self, test_name: str, test_function, models: List[str], *args) -> List[Dict]:
        """é€šç”¨æµ‹è¯•æ‰§è¡Œå™¨"""
        if not models:
            print(f"\nè·³è¿‡ {test_name} æµ‹è¯•ï¼šæ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ¨¡å‹ã€‚")
            return []
            
        print(f"\n{'='*20} å¼€å§‹ {test_name} æµ‹è¯• {'='*20}")
        print(f"å¾…æµ‹æ¨¡å‹: {', '.join(models)}")
        
        tasks = [test_function(model, *args) for model in models]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        processed_results = []
        for i, res in enumerate(results):
            if isinstance(res, Exception):
                processed_results.append({
                    "model": models[i], "success": False, "error": str(res)
                })
            else:
                processed_results.append(res)
        return processed_results

    def save_results_to_file(self, all_results: Dict[str, List[Dict]], filename: str = "test_results.json"):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(all_results, f, ensure_ascii=False, indent=2)
            print(f"\nâœ… æµ‹è¯•å®Œæˆï¼è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°æ–‡ä»¶: {filename}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

    async def run_comprehensive_test(self):
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        print("ğŸš€ å¼€å§‹ LLM ä»£ç†æ¨¡å‹ç»¼åˆæµ‹è¯•")
        print(f"æ€»æ¨¡å‹æ•°é‡: {len(self.models)}")
        print(f"æµ‹è¯•é—®é¢˜æ•°é‡: {len(self.test_questions)}")
        print(f"æ¨¡å‹åˆ—è¡¨: {', '.join(self.models)}")
        
        all_results = {
            "standard_text": [],
            "streaming_comparison": [],
            "multimodal": [],
            "tool_call": []
        }
        
        # 1. æ ‡å‡†æ–‡æœ¬æµ‹è¯•
        for i, question in enumerate(self.test_questions, 1):
            print(f"\nğŸ“ æ ‡å‡†æµ‹è¯• é—®é¢˜ {i}/{len(self.test_questions)}: {question}")
            results = await self._run_tests(f"æ ‡å‡†æµ‹è¯• (é—®é¢˜ {i})", self.test_single_model, self.models, question)
            all_results["standard_text"].extend(results)

        # 2. æµå¼ vs éæµå¼å¯¹æ¯”æµ‹è¯•
        if self.streaming_test_models:
            question = self.test_questions[0]
            print(f"\nğŸ’¨ å¼€å§‹æµå¼/éæµå¼å¯¹æ¯”æµ‹è¯• (é—®é¢˜: {question})")
            
            streaming_tasks = [self.test_streaming_single_model(model, question) for model in self.streaming_test_models]
            non_streaming_tasks = [self.test_single_model(model, question) for model in self.streaming_test_models]
            
            results = await asyncio.gather(*streaming_tasks, *non_streaming_tasks, return_exceptions=True)
            all_results["streaming_comparison"] = [res for res in results if not isinstance(res, Exception)]

        # 3. å¤šæ¨¡æ€æµ‹è¯•
        results = await self._run_tests("å¤šæ¨¡æ€", self.test_multimodal_single_model, self.multimodal_models)
        all_results["multimodal"] = results

        # 4. å·¥å…·è°ƒç”¨æµ‹è¯•
        results = await self._run_tests("å·¥å…·è°ƒç”¨", self.test_tool_call_single_model, self.tool_call_models)
        all_results["tool_call"] = results
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        self.save_results_to_file(all_results)
        
        # æ‰“å°æ‰€æœ‰æµ‹è¯•çš„æœ€ç»ˆæ‘˜è¦
        self.print_final_summary(all_results)

    def print_final_summary(self, all_results: Dict[str, List[Dict]]):
        """æ‰“å°æ‰€æœ‰æµ‹è¯•çš„æœ€ç»ˆæ‘˜è¦"""
        print("\n" + "ğŸ¯" * 40)
        print(" " * 30 + "æœ€ç»ˆæµ‹è¯•æ‘˜è¦")
        print("ğŸ¯" * 40)

        for test_type, results in all_results.items():
            if not results:
                continue

            print(f"\n--- {test_type.replace('_', ' ').upper()} ---")
            successful_models = [r for r in results if r.get('success')]
            failed_models = [r for r in results if not r.get('success')]
            
            print(f"æ€»æµ‹è¯•æ•°: {len(results)}")
            print(f"æˆåŠŸ: {len(successful_models)}, å¤±è´¥: {len(failed_models)}")

            if successful_models:
                # æŒ‰å“åº”æ—¶é—´æ’åºå¹¶æ‰“å°Top 3
                sorted_models = sorted(successful_models, key=lambda x: x.get('response_time', float('inf')))
                print("æ€§èƒ½æœ€ä½³ (Top 3):")
                for i, r in enumerate(sorted_models[:3]):
                    model_info = f"{i+1}. {r['model']:<30} | å“åº”æ—¶é—´: {r['response_time']:.2f}s"
                    if 'time_to_first_token' in r:
                        model_info += f" | TTFT: {r.get('time_to_first_token', 0):.2f}s"
                    if 'tool_calls' in r and r['tool_calls']:
                         model_info += f" | å·¥å…·è°ƒç”¨: âœ…"
                    print(model_info)

            if failed_models:
                print("å¤±è´¥çš„æ¨¡å‹:")
                for r in failed_models:
                    print(f"  - {r['model']}: {r.get('error', 'N/A')}")
        print("\n" + "ğŸ¯" * 40)


async def main():
    """ä¸»å‡½æ•°"""
    tester = ModelTester()
    await tester.run_comprehensive_test()


if __name__ == "__main__":
    asyncio.run(main()) 
